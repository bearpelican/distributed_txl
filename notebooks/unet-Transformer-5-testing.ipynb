{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from fastai import *        # Quick accesss to most common functionality\n",
    "# from fastai.text import *   # Quick accesss to NLP functionality\n",
    "import html\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.transformer import tfmer_lm_config, GeLU, init_transformer, MultiHeadAttention, PositionalEncoding\n",
    "from fastai.text.models.awd_lstm import RNNDropout\n",
    "from fastai.text.learner import LanguageLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path.home()/'data/wikitext-2-raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basic_data import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 512\n",
    "data = load_data(PATH, bs=16, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = 'xxmask'\n",
    "vocab = data.vocab\n",
    "vocab.itos.append(MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_range = (0, len(data.vocab.itos))\n",
    "from fastai.text.transform import *\n",
    "pad_idx = data.vocab.stoi[PAD]\n",
    "mask_idx = data.vocab.stoi[MASK]\n",
    "def bert_tfm(b, word_range=word_range, pad_idx=pad_idx, \n",
    "             mask_idx=mask_idx, p=0.2):\n",
    "    # p = replacement probability\n",
    "    x_lm,y_lm = b\n",
    "    x_msk,y_msk = x_lm.clone(),x_lm.clone() # x, x\n",
    "#     x,y = x.clone(),y.clone()\n",
    "    rand = torch.rand(x_msk.shape, device=x_lm.device)\n",
    "    y_msk[rand > p] = pad_idx\n",
    "    x_msk[rand <= (p*.8)] = mask_idx # 80% = mask\n",
    "    wrong_word = (rand > (p*.8)) & (rand <= (p*.9)) # 10% = wrong word\n",
    "    x_msk[wrong_word] = torch.randint(*word_range, [wrong_word.sum().item()], device=x_lm.device)\n",
    "    return x_msk, y_msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_dl.tfms = [bert_tfm]\n",
    "data.valid_dl.tfms = [bert_tfm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc_ignore_pad(input:Tensor, targ:Tensor, pad_idx=pad_idx)->Rank0Tensor:\n",
    "    n = targ.shape[0]\n",
    "    input = input.argmax(dim=-1).view(n,-1)\n",
    "    targ = targ.view(n,-1)\n",
    "    mask = targ != pad_idx\n",
    "    return (input[mask]==targ[mask]).float().mean()\n",
    "\n",
    "def bert_acc(input:Tensor, b_t:Tensor)->Rank0Tensor:\n",
    "    return acc_ignore_pad(input, b_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearDecoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int, n_hid:int, tie_encoder:nn.Module=None, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs, outputs = input\n",
    "        decoded = self.decoder(outputs[-1])\n",
    "        return decoded, raw_outputs, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.mhra = MultiHeadAttention(n_heads, d_model, d_head, bias=bias)\n",
    "        self.ff   = feed_forward(d_model, d_inner)\n",
    "    \n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs): \n",
    "        attn = self.mhra(x, mask=mask, **kwargs)\n",
    "        res = self.ff(attn)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def feed_forward(d_model:int, d_ff:int, ff_p:float=0., act=nn.ReLU, double_drop:bool=True):\n",
    "def feed_forward(d_model:int, d_inner:int, ff_p:float=0.1):\n",
    "    layers = [\n",
    "        nn.Linear(d_model, d_inner), \n",
    "        GeLU(),\n",
    "        nn.Linear(d_inner, d_model), \n",
    "        nn.Dropout(ff_p), \n",
    "        MergeLayer(),\n",
    "        nn.LayerNorm(d_model)\n",
    "    ]\n",
    "    return SequentialEx(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleLayer(nn.Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, d_model:int, n_heads:int, d_head:int, ff_p:float=0.1, downsample=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mhra = MultiHeadAttention(n_heads, d_model, d_head, bias=True)\n",
    "        \n",
    "        d_inner = d_model*4\n",
    "        if downsample:\n",
    "            d_out = d_model * 2\n",
    "            self.downblock = nn.Conv1d(d_model, d_out, (2), stride=2)\n",
    "        else:\n",
    "            self.downblock = None\n",
    "            d_out = d_model\n",
    "            \n",
    "        self.ln1 = nn.Linear(d_out, d_inner)\n",
    "        self.act = GeLU()\n",
    "        self.ln2 = nn.Linear(d_inner, d_out)\n",
    "        self.drop = nn.Dropout(ff_p)\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "\n",
    "    \n",
    "    def forward(self, x:Tensor):\n",
    "        x_attn = self.mhra(x)\n",
    "        x = x_attn\n",
    "        \n",
    "        if self.downblock:\n",
    "            x_p = x.permute(0, 2, 1)\n",
    "            x_d = self.downblock(x_p) # bptt x emb x bptt\n",
    "            x = x_d.permute(0, 2, 1)\n",
    "        \n",
    "        x1 = self.ln2(self.act(self.ln1(x)))\n",
    "        \n",
    "        x2 = x + x1\n",
    "        \n",
    "        return self.norm(self.drop(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleLayer(nn.Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, d_model:int, n_heads:int, d_head:int, ff_p:float=0.1, upsample=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mhra = MultiHeadAttention(n_heads, d_model, d_head, bias=True)\n",
    "        \n",
    "        d_inner = d_model*4\n",
    "        if upsample:\n",
    "            d_out = d_model // 2\n",
    "            self.upblock = nn.ConvTranspose1d(d_model, d_out, (2), stride=2)\n",
    "        else:\n",
    "            d_out = d_model\n",
    "            self.upblock = None\n",
    "            \n",
    "        self.ln1 = nn.Linear(d_out, d_inner)\n",
    "        self.act = GeLU()\n",
    "        self.ln2 = nn.Linear(d_inner, d_out)\n",
    "        self.drop = nn.Dropout(ff_p)\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "\n",
    "    \n",
    "    def forward(self, x:Tensor, x_skip:Tensor):\n",
    "        x_attn = self.mhra(x)\n",
    "        x = x_attn\n",
    "        \n",
    "        if self.upblock:\n",
    "            x_p = x.permute(0, 2, 1)\n",
    "            x_u = self.upblock(x_p) # bptt x emb x bptt\n",
    "            x = x_u.permute(0, 2, 1)\n",
    "            \n",
    "        x1 = self.ln2(self.act(self.ln1(x)))\n",
    "        x2 = self.norm(self.drop(x1))\n",
    "        return x2 + x_skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConv(nn.Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, vocab_sz:int, ctx_len:int, n_layers:int, n_heads:int, d_model:int, d_head:int, \n",
    "                 embed_p:float=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Embedding(vocab_sz, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.drop_emb = nn.Dropout(embed_p)\n",
    "#         self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner) for k in range(n_layers)])\n",
    "        self.a1 = DownsampleLayer(d_model, n_heads, d_head)\n",
    "        self.a2 = DownsampleLayer(d_model*2, n_heads, d_head)\n",
    "        self.a3 = DownsampleLayer(d_model*4, n_heads, d_head, downsample=False)\n",
    "        self.a4 = UpsampleLayer(d_model*4, n_heads, d_head)\n",
    "        self.a5 = UpsampleLayer(d_model*2, n_heads, d_head)\n",
    "    \n",
    "    def reset(self): pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, x_len = x.size()\n",
    "        pos = torch.arange(0, x_len, device=x.device, dtype=x.dtype).float()\n",
    "        inp = self.drop_emb(self.encoder(x) + self.pos_enc(pos)[None]) #.mul_(self.d_model ** 0.5)\n",
    "#         mask = torch.triu(x.new_ones(x_len, x_len), diagonal=1).byte()[None,None]\n",
    "        #[None,:,:None] for einsum implementation of attention\n",
    "#         for layer in self.layers: inp = layer(inp, mask=mask)\n",
    "#         print('Inp:', inp.shape)\n",
    "        x1 = self.a1(inp)\n",
    "#         print('x1:', x1.shape)\n",
    "        x2 = self.a2(x1)\n",
    "#         print('x2:', x2.shape)\n",
    "        x3 = self.a3(x2)\n",
    "        x4 = self.a4(x3, x1)\n",
    "        x5 = self.a5(x4, inp)\n",
    "        \n",
    "#         print(x1.shape, x2.shape, x3.shape, x4.shape)\n",
    "        return ([x5],[x5]) #For the LinearDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBase(nn.Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, vocab_sz:int, ctx_len:int, n_layers:int, n_heads:int, d_model:int, d_head:int, \n",
    "                 embed_p:float=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Embedding(vocab_sz, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.drop_emb = nn.Dropout(embed_p)\n",
    "        self.layers = nn.ModuleList([DownsampleLayer(d_model, d_head, n_heads, downsample=False) for k in range(n_layers)])\n",
    "    \n",
    "    def reset(self): pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, x_len = x.size()\n",
    "        pos = torch.arange(0, x_len, device=x.device, dtype=x.dtype).float()\n",
    "        inp = self.drop_emb(self.encoder(x) + self.pos_enc(pos)[None]) #.mul_(self.d_model ** 0.5)\n",
    "#         mask = torch.triu(x.new_ones(x_len, x_len), diagonal=1).byte()[None,None]\n",
    "        for layer in self.layers: inp = layer(inp)\n",
    "        return ([inp],[inp]) #For the LinearDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sz = len(data.vocab.itos)\n",
    "# config = tfmer_lm_config.copy(); config\n",
    "config = {\n",
    "    'ctx_len': bptt,\n",
    "    'n_layers': 4,\n",
    "    'n_heads': 4,\n",
    "    'd_model': 128,\n",
    "    'd_head': 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb,yb = data.one_batch(cpu=False)\n",
    "# model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerBase(vocab_sz, **config)\n",
    "decoder = LinearDecoder(vocab_sz, config['d_model'], tie_encoder=encoder.encoder, bias=False)\n",
    "model = nn.Sequential(encoder, decoder)\n",
    "model.reset = lambda: True\n",
    "model.apply(init_transformer)\n",
    "learn = LanguageLearner(data, model, loss_func=CrossEntropyFlat(ignore_index=pad_idx))\n",
    "learn.metrics = [bert_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learn.lr_find(num_it=300)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.612016</td>\n",
       "      <td>6.361951</td>\n",
       "      <td>0.141403</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.554222</td>\n",
       "      <td>6.308731</td>\n",
       "      <td>0.144479</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.529598</td>\n",
       "      <td>6.302287</td>\n",
       "      <td>0.141995</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.522533</td>\n",
       "      <td>6.286721</td>\n",
       "      <td>0.142608</td>\n",
       "      <td>00:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerConv(vocab_sz, **config)\n",
    "decoder = LinearDecoder(vocab_sz, config['d_model'], tie_encoder=encoder.encoder, bias=False)\n",
    "model = nn.Sequential(encoder, decoder)\n",
    "model.reset = lambda: True\n",
    "model.apply(init_transformer)\n",
    "learn = LanguageLearner(data, model, loss_func=CrossEntropyFlat(ignore_index=pad_idx))\n",
    "learn.metrics = [bert_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.818747</td>\n",
       "      <td>6.578912</td>\n",
       "      <td>0.137784</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.719836</td>\n",
       "      <td>6.413465</td>\n",
       "      <td>0.171426</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.587164</td>\n",
       "      <td>6.287138</td>\n",
       "      <td>0.180866</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.486319</td>\n",
       "      <td>6.222453</td>\n",
       "      <td>0.181621</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(4, 1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
