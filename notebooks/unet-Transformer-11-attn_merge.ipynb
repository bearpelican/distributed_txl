{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from fastai import *        # Quick accesss to most common functionality\n",
    "# from fastai.text import *   # Quick accesss to NLP functionality\n",
    "import html\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.transformer import *\n",
    "from fastai.text.models.transformer import init_transformer\n",
    "from fastai.text.models.awd_lstm import RNNDropout, LinearDecoder\n",
    "from fastai.text.learner import LanguageLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path.home()/'data/wikitext-103-raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basic_data import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 512\n",
    "data = load_data(PATH, bs=16, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = 'xxmask'\n",
    "vocab = data.vocab\n",
    "vocab.itos.append(MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_range = (0, len(data.vocab.itos))\n",
    "from fastai.text.transform import *\n",
    "pad_idx = data.vocab.stoi[PAD]\n",
    "mask_idx = data.vocab.stoi[MASK]\n",
    "def bert_tfm(b, word_range=word_range, pad_idx=pad_idx, \n",
    "             mask_idx=mask_idx, p=0.2):\n",
    "    # p = replacement probability\n",
    "    x_lm,y_lm = b\n",
    "    x_msk,y_msk = x_lm.clone(),x_lm.clone() # x, x\n",
    "    rand = torch.rand(x_msk.shape, device=x_lm.device)\n",
    "    y_msk[rand > p] = pad_idx\n",
    "    x_msk[rand <= (p*.8)] = mask_idx # 80% = mask\n",
    "    wrong_word = (rand > (p*.8)) & (rand <= (p*.9)) # 10% = wrong word\n",
    "    x_msk[wrong_word] = torch.randint(*word_range, [wrong_word.sum().item()], device=x_lm.device)\n",
    "    return x_msk, y_msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_dl.tfms = [bert_tfm]\n",
    "data.valid_dl.tfms = [bert_tfm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc_ignore_pad(input:Tensor, targ:Tensor, pad_idx=pad_idx)->Rank0Tensor:\n",
    "    n = targ.shape[0]\n",
    "    input = input.argmax(dim=-1).view(n,-1)\n",
    "    targ = targ.view(n,-1)\n",
    "    mask = targ != pad_idx\n",
    "    return (input[mask]==targ[mask]).float().mean()\n",
    "\n",
    "def bert_acc(input:Tensor, b_t:Tensor)->Rank0Tensor:\n",
    "    return acc_ignore_pad(input, b_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int, n_hid:int, output_p:float=0.0, tie_encoder:nn.Module=None, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        output = self.output_dp(input)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, d_attn:int, n_heads:int, d_head:int, d_out:int=None, bias:bool=True, ff_p=0.0):\n",
    "        super().__init__()\n",
    "        if d_out is None: d_out = d_attn\n",
    "        d_inner = d_out*4\n",
    "        self.mhra = MultiHeadRelativeAttention(n_heads, d_attn, d_head, bias=bias)\n",
    "#         self.mhra = MultiHeadAttention(n_heads, d_attn, d_head, bias=bias)\n",
    "\n",
    "        self.ln = nn.Sequential(*[\n",
    "            nn.Linear(d_out, d_inner), \n",
    "            GeLU(),\n",
    "            nn.Linear(d_inner, d_out),\n",
    "            nn.Dropout(ff_p), \n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "        \n",
    "        self.pos_enc = PositionalEncoding(d_attn)\n",
    "        self.x_p = None\n",
    "        \n",
    "    def attn(self, x:Tensor, **kwargs):\n",
    "#         return self.mhra(x)\n",
    "        if self.x_p is None:\n",
    "            pos = torch.arange(x.shape[1]-1, -1, -1, device=x.device, dtype=x.dtype)\n",
    "            self.x_p = self.pos_enc(pos)\n",
    "        u, v = 0, 0\n",
    "        return self.mhra(x, r=self.x_p, u=u, v=v)\n",
    "        \n",
    "    def forward(self, x:Tensor, **kwargs): \n",
    "        x_attn = self.attn(x, **kwargs)\n",
    "        res = self.ln(x_attn) + x_attn\n",
    "        return self.norm(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleLayer(TransformerBlock):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, d_model:int, n_heads:int, d_head:int):\n",
    "        d_out = d_model * 2\n",
    "        super().__init__(d_out, n_heads, d_head)\n",
    "        self.downblock = nn.Conv1d(d_model, d_out, (2), stride=2)\n",
    "            \n",
    "    def forward(self, x:Tensor, **kwargs):\n",
    "        x_d = self.downblock(x.permute(0, 2, 1)).permute(0, 2, 1) # bptt x emb x bptt\n",
    "        \n",
    "        x_attn = self.attn(x_d, **kwargs)\n",
    "        \n",
    "        x1 = self.ln(x_attn)\n",
    "        \n",
    "        return self.norm(x_attn + x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleLayer(TransformerBlock):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, d_model:int, n_heads:int, d_head:int):\n",
    "        d_out = d_model // 2\n",
    "        super().__init__(d_out, n_heads, d_head)\n",
    "        self.upblock = nn.ConvTranspose1d(d_model, d_out, (2), stride=2)\n",
    "    \n",
    "    def forward(self, x:Tensor, x_skip:Tensor, **kwargs):\n",
    "        x_u = self.upblock(x.permute(0, 2, 1)).permute(0, 2, 1) # bptt x emb x bptt\n",
    "        \n",
    "        x_attn = self.attn(x_u, **kwargs)\n",
    "        \n",
    "        x1 = self.ln(x_attn) + x_attn\n",
    "        \n",
    "        return self.norm(x1 + x_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    \"Embedding + positional encoding + dropout\"\n",
    "    def __init__(self, vocab_sz:int, d_model:int, embed_p:float=0.):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_sz, d_model)\n",
    "        # See https://arxiv.org/abs/1711.09160\n",
    "        with torch.no_grad(): trunc_normal_(self.embed.weight, std=0.01)\n",
    "        self.drop = nn.Dropout(embed_p)\n",
    "    \n",
    "    def forward(self, inp, pos_forward=False):\n",
    "        emb = self.drop(self.embed(inp))\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConv(nn.Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, vocab_sz:int, n_layers:int, d_model:int, n_heads:int, d_head:int, \n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_sz, d_model)\n",
    "        self.a1 = DownsampleLayer(d_model, n_heads, d_head)\n",
    "        self.a2 = DownsampleLayer(d_model*2, n_heads, d_head)\n",
    "        self.a3 = TransformerBlock(d_model*4, n_heads, d_head)\n",
    "        self.a4 = UpsampleLayer(d_model*4, n_heads, d_head)\n",
    "        self.a5 = UpsampleLayer(d_model*2, n_heads, d_head)\n",
    "    \n",
    "    def reset(self): pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp = self.embed(x)\n",
    "        x1 = self.a1(inp)\n",
    "        x2 = self.a2(x1)\n",
    "        x3 = self.a3(x2)\n",
    "        x4 = self.a4(x3, x1)\n",
    "        x5 = self.a5(x4, inp)\n",
    "        return x5 #For the LinearDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBase(nn.Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, vocab_sz:int, n_layers:int, d_model:int, n_heads:int, d_head:int, \n",
    "                 embed_p:float=0.0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_sz, d_model)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_model, n_heads, d_head) for k in range(n_layers)])\n",
    "    \n",
    "    def reset(self): pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp = self.embed(x)\n",
    "        for layer in self.layers: inp = layer(inp)\n",
    "        return inp #For the LinearDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sz = len(data.vocab.itos)\n",
    "# config = tfmer_lm_config.copy(); config\n",
    "config = {\n",
    "    'n_layers': 5,\n",
    "    'n_heads': 4,\n",
    "    'd_model': 128,\n",
    "    'd_head': 32,\n",
    "    'embed_p': 0.0 # Embed p needs to be 0 to match bert-mask baseline\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerConv(vocab_sz, **config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=encoder.embed.embed)\n",
    "model = nn.Sequential(encoder, decoder)\n",
    "model.reset = lambda: True\n",
    "model.apply(init_transformer)\n",
    "learn = LanguageLearner(data, model, loss_func=CrossEntropyFlat(ignore_index=pad_idx))\n",
    "learn.metrics = [bert_acc]\n",
    "learn.callbacks = []\n",
    "# learn.to_fp16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.584596</td>\n",
       "      <td>3.465516</td>\n",
       "      <td>0.452597</td>\n",
       "      <td>31:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.037114</td>\n",
       "      <td>2.943001</td>\n",
       "      <td>0.508547</td>\n",
       "      <td>31:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Fastai TXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.text.learner import get_language_model\n",
    "# encoder = TransformerXL(vocab_sz, d_inner=config['d_model']*4, ctx_len=bptt, **config)\n",
    "# decoder = LinearDecoder(vocab_sz, config['d_model'], output_p=0.0, tie_encoder=encoder.encoder)\n",
    "# model = nn.Sequential(encoder, decoder)\n",
    "# model.reset = lambda: True\n",
    "# model.apply(init_transformer)\n",
    "\n",
    "# learn = LanguageLearner(data, model, loss_func=CrossEntropyFlat(ignore_index=pad_idx))\n",
    "# learn.metrics = [bert_acc]\n",
    "# # learn.to_fp16();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Transformer Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerBase(vocab_sz, **config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=encoder.embed.embed)\n",
    "model = nn.Sequential(encoder, decoder)\n",
    "model.reset = lambda: True\n",
    "model.apply(init_transformer)\n",
    "learn = LanguageLearner(data, model, loss_func=CrossEntropyFlat(ignore_index=pad_idx))\n",
    "learn.callbacks = []\n",
    "learn.metrics = [bert_acc]\n",
    "# learn.to_fp16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.090801</td>\n",
       "      <td>2.985096</td>\n",
       "      <td>0.509065</td>\n",
       "      <td>33:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.774094</td>\n",
       "      <td>2.696457</td>\n",
       "      <td>0.541146</td>\n",
       "      <td>33:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(num_it=300)\n",
    "# learn.recorder.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
