{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from fastai import *        # Quick accesss to most common functionality\n",
    "# from fastai.text import *   # Quick accesss to NLP functionality\n",
    "import html\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.transformer import tfmer_lm_config, GeLU, init_transformer\n",
    "from fastai.text.models.awd_lstm import RNNDropout\n",
    "from fastai.text.learner import LanguageLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path.home()/'data/wikitext-2-raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basic_data import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert task - dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 256\n",
    "data = load_data(PATH, bs=8, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = 'xxmask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = data.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.itos.append(MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>team of writers handled the script . xxmaj the game 's opening theme was sung by xxmaj may ' n . xxbos xxmaj it met with positive sales in xxmaj japan , and was praised by both xxmaj japanese and western critics . xxmaj after release , it received downloadable content , along with an expanded edition in xxmaj november of that year . xxmaj it was also adapted into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>, which relentlessly and brutally and xxunk keeps these vicious , murderous wars . xxmaj it is a vandal state . xxmaj there is a xxmaj russian writer who once described vandal states as xxmaj genghis xxmaj khan with a telegraph . xxmaj israel is xxmaj genghis xxmaj khan with a computer . i feel no emotion of affinity with that state . i have some good friends and their</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>the 5th xxmaj division to envelop xxup un troops and push them back to xxmaj pusan . xxmaj the 766th was not reinforced ; xxmaj north xxmaj korean planners intended it to move unseen around the xxup un lines while the majority of the xxup un and xxmaj north xxmaj korean troops were locked in fighting around xxmaj taegu and the xxmaj xxunk xxmaj bulge . xxbos xxmaj by this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>at least two \" facilitating subjects \" ; the average point score per qualification was xxunk , equating to a xxup c- grade , and the average point score per student was xxunk . xxmaj the xxmaj sunday xxmaj times ranked xxmaj carre 's 101st ( 49th amongst state schools ) in the xxmaj midlands and 750th nationally based on a - xxmaj level and xxup gcse performance in 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>mfume , the president of the xxmaj national xxmaj association for the xxmaj advancement of xxmaj colored xxmaj people ( xxup naacp ) , would run . xxmaj mfume had previously served on the xxmaj baltimore xxmaj city xxmaj council and in the xxmaj united xxmaj states xxmaj house of xxmaj representatives . xxmaj schmoke called the race \" his to lose \" . xxmaj however , xxmaj mfume lived</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    9,  3531,  5611,  ...,  1105,   622,    29],\n",
       "         [16077,    19,  4075,  ..., 17572,    60,     9],\n",
       "         [   12,     9,     6,  ...,   344,    12,     5],\n",
       "         ...,\n",
       "         [    9, 13634,   749,  ...,    54,    16,     5],\n",
       "         [ 1228,   367,   785,  ...,    16,  2218,    12],\n",
       "         [   15,  4549,     9,  ...,    21,  1107,   869]]),\n",
       " tensor([[ 3531,  5611,  3398,  ...,   622,    29,     6],\n",
       "         [   19,  4075,    16,  ...,    60,     9,   465],\n",
       "         [    9,     6,  6630,  ...,    12,     5,   113],\n",
       "         ...,\n",
       "         [13634,   749,    23,  ...,    16,     5,   307],\n",
       "         [  367,   785,   321,  ...,  2218,    12,  1636],\n",
       "         [ 4549,     9, 17780,  ...,  1107,   869,    11]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_range = (0, len(data.vocab.itos))\n",
    "from fastai.text.transform import *\n",
    "pad_idx = data.vocab.stoi[PAD]\n",
    "mask_idx = data.vocab.stoi[MASK]\n",
    "def mask_tfm(b, word_range=word_range, pad_idx=pad_idx, \n",
    "             mask_idx=mask_idx, p=0.2):\n",
    "    # p = replacement probability\n",
    "    x_lm,y_lm = b\n",
    "    x_msk,y_msk = x_lm.clone(),x_lm.clone() # x, x\n",
    "#     x,y = x.clone(),y.clone()\n",
    "    rand = torch.rand(x_msk.shape, device=x_lm.device)\n",
    "    y_msk[rand > p] = pad_idx\n",
    "    x_msk[rand <= (p*.8)] = mask_idx # 80% = mask\n",
    "    wrong_word = (rand > (p*.8)) & (rand <= (p*.9)) # 10% = wrong word\n",
    "    x_msk[wrong_word] = torch.randint(*word_range, [wrong_word.sum().item()], device=x_lm.device)\n",
    "    return x_msk, (y_msk, y_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nw_tfm(b, pad_idx=pad_idx, \n",
    "             mask_idx=mask_idx, p=0.2):\n",
    "    # p = replacement probability\n",
    "    x_lm,y_lm = b\n",
    "    \n",
    "    y_msk = y_lm.clone() # x, x\n",
    "    rand = torch.rand(x_lm.shape, device=x_lm.device)\n",
    "    y_msk[rand > p] = pad_idx\n",
    "    msk_idxs = rand <= p\n",
    "    return (x_lm, msk_idxs), (y_msk, y_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnd_tfm(b):\n",
    "    r = random.randint(0, 1)\n",
    "    if r == 0:\n",
    "        return nw_tfm(b)\n",
    "    return mask_tfm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_tfm(b):\n",
    "    x_lm, y_lm = b\n",
    "    x_msk, ys = mask_tfm(b)\n",
    "    y_msk, _ = ys\n",
    "    return (x_msk,x_lm), (y_msk, y_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.train_dl.add_tfm(rnd_tfm)\n",
    "# data.valid_dl.add_tfm(val_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    6, 13297,    28,  ...,   965,   622,    41],\n",
       "        [  465,    15,  2687,  ...,   342,     9,     6],\n",
       "        [  113,     5,  3293,  ...,     2,    17,     5],\n",
       "        ...,\n",
       "        [  307,     5,   335,  ...,   123,     9,  2764],\n",
       "        [ 1636,   380,   858,  ...,     9,     5,  1405],\n",
       "        [   11,     5,     9,  ...,   556,     5,  8556]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13297,    28,  2619,  ...,   622,    41,  1127],\n",
       "        [   15,  2687,     5,  ...,     9,     6, 12753],\n",
       "        [    5,  3293,   672,  ...,    17,     5,  1357],\n",
       "        ...,\n",
       "        [    5,   335,    10,  ...,     9,  2764,    10],\n",
       "        [  380,   858,    11,  ...,     5,  1405,     5],\n",
       "        [    5,     9,   147,  ...,     5,  8556,    24]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = data.one_batch(ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,    17,     5,  ...,   830, 10584,    14],\n",
       "        [  245,  1266,   312,  ...,     9,  2166,   705],\n",
       "        [   16,   100,    12,  ...,    32,     5,  3866],\n",
       "        ...,\n",
       "        [   55,     5,     0,  ...,  5760,    15,     9],\n",
       "        [   13,  2957,    71,  ...,  3168,  2630,    14],\n",
       "        [28656,   163,    42,  ...,    32,     5,  7136]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   17,     5, 10034,  ..., 10584,    14,   356],\n",
       "        [ 1266,   312,  1400,  ...,  2166,   705,    10],\n",
       "        [  100,    12,   313,  ...,     5,  3866,    10],\n",
       "        ...,\n",
       "        [    5,     0,  1283,  ...,    15,     9,   658],\n",
       "        [ 2957,    71,    36,  ...,  2630,    14,     5],\n",
       "        [  163,    42,    68,  ...,     5,  7136,     5]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder(nn.Module):\n",
    "    \"Embedding + positional encoding + dropout\"\n",
    "    def __init__(self, vocab_sz:int, emb_sz:int, embed_p:float=0., pad_idx=pad_idx):\n",
    "        super().__init__()\n",
    "        self.emb_sz = emb_sz\n",
    "        \n",
    "#         self.embed = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_idx)        \n",
    "        self.embed = nn.Embedding(vocab_sz, emb_sz)\n",
    "        # See https://arxiv.org/abs/1711.09160\n",
    "        with torch.no_grad(): trunc_normal_(self.embed.weight, std=0.01)\n",
    "        self.drop = nn.Dropout(embed_p)\n",
    "    \n",
    "    def forward(self, inp, pos_forward=False):\n",
    "        emb = self.drop(self.embed(inp))\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"TransformerXL model: https://arxiv.org/abs/1901.02860.\"\n",
    "    def __init__(self, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., bias:bool=False, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadRelativeAttention,\n",
    "                 learned_pos_enc:bool=False, mask:bool=True, mem_len:int=0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.u = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.v = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.mem_len,self.n_layers,self.d_model,self.mask = mem_len,n_layers,d_model,mask\n",
    "        self.init = False\n",
    "        self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                      attn_cls=attn_cls) for k in range(n_layers)])\n",
    "        \n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"Reset the internal memory.\"\n",
    "        self.hidden = [next(self.parameters()).data.new(0) for i in range(self.n_layers+1)]\n",
    "\n",
    "    def _update_mems(self, hids):\n",
    "        if not getattr(self, 'hidden', False): return None\n",
    "        assert len(hids) == len(self.hidden), 'len(hids) != len(self.hidden)'\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(hids)):\n",
    "                cat = torch.cat([self.hidden[i], hids[i]], dim=1)\n",
    "                self.hidden[i] = cat[:,-self.mem_len:].detach()\n",
    "    \n",
    "    def select_hidden(self, idxs): self.hidden = [h[idxs] for h in self.hidden]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #The hidden state has to be initiliazed in the forward pass for nn.DataParallel\n",
    "        if self.mem_len > 0 and not self.init: \n",
    "            self.reset()\n",
    "            self.init = True\n",
    "        bs,x_len,emb_sz = x.size()\n",
    "        \n",
    "        inp = x\n",
    "        \n",
    "        m_len = self.hidden[0].size(1) if hasattr(self, 'hidden') and len(self.hidden[0].size()) > 1 else 0\n",
    "        seq_len = m_len + x_len\n",
    "        mask = torch.triu(x.new_ones(x_len, seq_len).long(), diagonal=1+m_len).byte()[None,None] if self.mask else None\n",
    "        \n",
    "        hids = []\n",
    "        pos = torch.arange(seq_len-1, -1, -1, device=inp.device, dtype=inp.dtype)\n",
    "        pos_enc = self.pos_enc(pos)\n",
    "        hids.append(inp)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            mem = self.hidden[i] if self.mem_len > 0 else None\n",
    "            inp = layer(inp, r=pos_enc, u=self.u, v=self.v, mask=mask, mem=mem)\n",
    "            hids.append(inp)\n",
    "        core_out = inp[:,-x_len:]\n",
    "        if self.mem_len > 0 : self._update_mems(hids)\n",
    "        return core_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertHead(nn.Module):\n",
    "    def __init__(self, embed, bert_encoder, nw_encoder, decoder, lm_only=False):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "        self.bert_encoder = bert_encoder\n",
    "        self.nw_encoder = nw_encoder\n",
    "        self.decoder = decoder\n",
    "        self.lm_only = lm_only\n",
    "        \n",
    "    \n",
    "    def forward(self, x, mask_idxs=None):\n",
    "        x_enc = self.embed(x)\n",
    "        \n",
    "        # Baseline\n",
    "        if self.lm_only:\n",
    "            nw_enc = self.nw_encoder(x_enc)\n",
    "            return self.decoder(nw_enc)\n",
    "        \n",
    "        # Validation - train separately\n",
    "        if not self.training:\n",
    "#         if True:\n",
    "            bert_enc = self.bert_encoder(x_enc)\n",
    "            \n",
    "            x_lm_enc = self.embed(mask_idxs)\n",
    "            nw_enc = self.nw_encoder(x_lm_enc)\n",
    "            return self.decoder(bert_enc), self.decoder(nw_enc)\n",
    "        \n",
    "        bert_first = mask_idxs is None # mask idxs tells us which embeddings to mask\n",
    "        if bert_first:\n",
    "            bert_enc = self.bert_encoder(x_enc)\n",
    "            nw_enc = self.nw_encoder(bert_enc)\n",
    "            return self.decoder(bert_enc), self.decoder(nw_enc)\n",
    "        else:\n",
    "            nw_enc = self.nw_encoder(x_enc)\n",
    "            nw_enc[mask_idxs] = embed(torch.tensor(mask_idx, device=x.device))\n",
    "            bert_enc = self.bert_encoder(nw_enc)\n",
    "            return self.decoder(bert_enc), self.decoder(nw_enc)\n",
    "    \n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int, n_hid:int, output_p:float, tie_encoder:nn.Module=None, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        output = self.output_dp(input)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sz = len(data.vocab.itos)\n",
    "# config = tfmer_lm_config.copy(); config\n",
    "config = {\n",
    "    'ctx_len': bptt,\n",
    "    'n_layers': 4,\n",
    "    'n_heads': 4,\n",
    "    'd_model': 128,\n",
    "    'd_head': 32,\n",
    "    'd_inner': 512,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLoss():\n",
    "    def __init__(self, pad_idx=pad_idx):\n",
    "        \"Loss mult - mask, NextWord, Seq2Seq, NextSent\"\n",
    "        self.index_loss = CrossEntropyFlat(ignore_index=pad_idx)\n",
    "        \n",
    "    def __call__(self, input:Tensor, bert_target:Tensor, lm_target:Tensor, **kwargs)->Rank0Tensor:\n",
    "        x_bert, x_lm = input\n",
    "        loss_bert = self.index_loss.__call__(x_bert, bert_target, **kwargs)\n",
    "        loss_lm = self.index_loss.__call__(x_lm, lm_target, **kwargs)\n",
    "#         print(loss_bert, loss_lm)\n",
    "        return loss_bert + loss_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc_ignore_pad(input:Tensor, targ:Tensor, pad_idx=pad_idx)->Rank0Tensor:\n",
    "    n = targ.shape[0]\n",
    "    input = input.argmax(dim=-1).view(n,-1)\n",
    "    targ = targ.view(n,-1)\n",
    "    mask = targ != pad_idx\n",
    "    return (input[mask]==targ[mask]).float().mean()\n",
    "\n",
    "def bert_acc(input:Tensor, b_t:Tensor, lm_t:Tensor)->Rank0Tensor:\n",
    "    x_bert, x_lm = input\n",
    "    return acc_ignore_pad(x_bert, b_t)\n",
    "def lm_acc(input:Tensor, b_t:Tensor, lm_t:Tensor)->Rank0Tensor:\n",
    "    x_bert, x_lm = input\n",
    "    return acc_ignore_pad(x_lm, lm_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedder(vocab_sz, config['d_model'])\n",
    "bert_encoder = Encoder(**config)\n",
    "bert_encoder.mask = False\n",
    "nw_encoder = Encoder(**config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=embed.embed, bias=False, output_p=False)\n",
    "bert_head = BertHead(embed, bert_encoder, nw_encoder, decoder)\n",
    "model = bert_head\n",
    "model.apply(init_transformer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_dl.add_tfm(rnd_tfm)\n",
    "# data.train_dl.add_tfm(mask_tfm)\n",
    "# data.train_dl.add_tfm(nw_tfm)\n",
    "# data.train_dl.add_tfm(val_tfm)\n",
    "data.valid_dl.add_tfm(val_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, model, loss_func=BertLoss(), clip=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics=[bert_acc, lm_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb,yb = data.one_batch(cpu=False)\n",
    "\n",
    "# out = model(*xb)\n",
    "\n",
    "# learn.loss_func(out, *yb)\n",
    "\n",
    "# learn.validate()\n",
    "\n",
    "# learn.pred_batch(batch=(xb,yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(num_it=500)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>lm_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.992301</td>\n",
       "      <td>11.077176</td>\n",
       "      <td>0.339810</td>\n",
       "      <td>0.148441</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.872988</td>\n",
       "      <td>11.870544</td>\n",
       "      <td>0.374889</td>\n",
       "      <td>0.127701</td>\n",
       "      <td>01:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Train Separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>lm_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.751541</td>\n",
       "      <td>9.352708</td>\n",
       "      <td>0.368044</td>\n",
       "      <td>0.268756</td>\n",
       "      <td>01:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.943863</td>\n",
       "      <td>8.725395</td>\n",
       "      <td>0.408181</td>\n",
       "      <td>0.285668</td>\n",
       "      <td>01:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Next Word only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedder(vocab_sz, config['d_model'])\n",
    "bert_encoder = Encoder(**config)\n",
    "nw_encoder = Encoder(**config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=embed.embed, bias=False, output_p=False)\n",
    "bert_head = BertHead(embed, bert_encoder, nw_encoder, decoder, lm_only=True)\n",
    "model = bert_head\n",
    "model.apply(init_transformer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def base_acc(input:Tensor, t1:Tensor)->Rank0Tensor:\n",
    "    return acc_ignore_pad(input, t1)\n",
    "learn.metrics=[base_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>base_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.700126</td>\n",
       "      <td>5.500681</td>\n",
       "      <td>0.212985</td>\n",
       "      <td>00:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.485490</td>\n",
       "      <td>5.370440</td>\n",
       "      <td>0.220364</td>\n",
       "      <td>00:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
