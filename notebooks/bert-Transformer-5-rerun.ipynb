{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from fastai import *        # Quick accesss to most common functionality\n",
    "# from fastai.text import *   # Quick accesss to NLP functionality\n",
    "import html\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.transformer import tfmer_lm_config, GeLU, init_transformer\n",
    "from fastai.text.models.awd_lstm import RNNDropout\n",
    "from fastai.text.learner import LanguageLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path.home()/'data/wikitext-2-raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basic_data import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert task - dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 512\n",
    "data = load_data(PATH, bs=16, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = 'xxmask'\n",
    "\n",
    "vocab = data.vocab\n",
    "\n",
    "vocab.itos.append(MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . xxmaj outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . xxmaj alongside the main story missions are character - specific sub missions relating to different squad members . xxmaj after the game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>xxmaj kettle xxmaj hole xxmaj pond and to the southwest of xxmaj secret xxmaj lake in a heavily forested region . xxmaj after xxmaj secret xxmaj lake , the highway curves to the north , crossing xxmaj oak xxmaj hill xxmaj road at another at - grade intersection . xxbos xxmaj shortly after the intersection with xxmaj oak xxmaj hill xxmaj road , xxmaj route 4 transitions from a divided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>hamas was seeking a diplomatic settlement with xxmaj israel . xxmaj he also condemned what he said was xxmaj israel 's refusal \" to abide by international law [ and ] to abide by the opinion of the international community \" to settle the conflict . xxbos \" i was of course happy to meet the xxmaj hizbullah people , because it is a point of view that is rarely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>june demonstrated a number of the major problems inherent in the xxmaj french and xxmaj british navies at the start of the xxmaj revolutionary xxmaj wars . xxmaj both admirals were faced with disobedience from their captains , along with ill - discipline and poor training among their shorthanded crews , and they failed to control their fleets effectively during the height of the combat . xxbos = = xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>of the xxup rok 3rd xxmaj division . xxmaj the xxmaj south xxmaj korean forces engaged the 766th forces around the village 's middle school with small - arms fire until noon . xxmaj at that point , xxmaj north xxmaj korean armored vehicles moved in to reinforce the 766th troops and drove the xxmaj south xxmaj koreans out of the village . xxbos xxmaj the village was strategically important</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1127,    15,     9,  ...,  2980,     5,  4042],\n",
       "         [ 3366,    15,   217,  ...,    12,  1653,    72],\n",
       "         [12753,    11,     5,  ..., 21301,   406,  1252],\n",
       "         ...,\n",
       "         [   10,     9,  1798,  ...,    15,     9,  1658],\n",
       "         [   24,   540,    10,  ..., 14090,    11,     5],\n",
       "         [   12,     5,  2994,  ...,   618,  9654,    26]]),\n",
       " tensor([[   15,     9,  2618,  ...,     5,  4042,    12],\n",
       "         [   15,   217,  8701,  ...,  1653,    72,  1402],\n",
       "         [   11,     5,    46,  ...,   406,  1252,    16],\n",
       "         ...,\n",
       "         [    9,  1798,    12,  ...,     9,  1658,    14],\n",
       "         [  540,    10,    23,  ...,    11,     5,    35],\n",
       "         [    5,  2994, 13419,  ...,  9654,    26,     5]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_range = (0, len(data.vocab.itos))\n",
    "from fastai.text.transform import *\n",
    "pad_idx = data.vocab.stoi[PAD]\n",
    "mask_idx = data.vocab.stoi[MASK]\n",
    "def bert_first_tfm(b, word_range=word_range, pad_idx=pad_idx, \n",
    "             mask_idx=mask_idx, p=0.2):\n",
    "    # p = replacement probability\n",
    "    x_lm,y_lm = b\n",
    "    x_msk,y_msk = x_lm.clone(),x_lm.clone() # x, x\n",
    "#     x,y = x.clone(),y.clone()\n",
    "    rand = torch.rand(x_msk.shape, device=x_lm.device)\n",
    "    y_msk[rand > p] = pad_idx\n",
    "    x_msk[rand <= (p*.8)] = mask_idx # 80% = mask\n",
    "    wrong_word = (rand > (p*.8)) & (rand <= (p*.9)) # 10% = wrong word\n",
    "    x_msk[wrong_word] = torch.randint(*word_range, [wrong_word.sum().item()], device=x_lm.device)\n",
    "    return x_msk, (y_msk, y_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_first_tfm(b, pad_idx=pad_idx, \n",
    "             mask_idx=mask_idx, p=0.2):\n",
    "    # p = replacement probability\n",
    "    x_lm,y_lm = b\n",
    "    \n",
    "    y_msk = y_lm.clone() # x, x\n",
    "    rand = torch.rand(x_lm.shape, device=x_lm.device)\n",
    "    y_msk[rand > p] = pad_idx\n",
    "    msk_idxs = rand <= p\n",
    "    return (x_lm, msk_idxs), (y_msk, y_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnd_tfm(b):\n",
    "    r = random.randint(0, 1)\n",
    "    if r == 0:\n",
    "        return lm_first_tfm(b)\n",
    "    return bert_first_tfm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_tfm(b):\n",
    "    x_lm, y_lm = b\n",
    "    x_msk, ys = bert_first_tfm(b)\n",
    "    y_msk, _ = ys\n",
    "    return (x_msk,x_lm), (y_msk, y_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder(nn.Module):\n",
    "    \"Embedding + positional encoding + dropout\"\n",
    "    def __init__(self, vocab_sz:int, emb_sz:int, embed_p:float=0., pad_idx=pad_idx):\n",
    "        super().__init__()\n",
    "        self.emb_sz = emb_sz\n",
    "        \n",
    "#         self.embed = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_idx)        \n",
    "        self.embed = nn.Embedding(vocab_sz, emb_sz)\n",
    "        # See https://arxiv.org/abs/1711.09160\n",
    "        with torch.no_grad(): trunc_normal_(self.embed.weight, std=0.01)\n",
    "        self.drop = nn.Dropout(embed_p)\n",
    "    \n",
    "    def forward(self, inp, pos_forward=False):\n",
    "        emb = self.drop(self.embed(inp))\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"TransformerXL model: https://arxiv.org/abs/1901.02860.\"\n",
    "    def __init__(self, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., bias:bool=False, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadRelativeAttention,\n",
    "                 learned_pos_enc:bool=False, mask:bool=True, mem_len:int=0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.u = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.v = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.mem_len,self.n_layers,self.d_model,self.mask = mem_len,n_layers,d_model,mask\n",
    "        self.init = False\n",
    "        self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                      attn_cls=attn_cls) for k in range(n_layers)])\n",
    "        \n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        \n",
    "        nn.init.normal_(self.u, 0., 0.02)\n",
    "        nn.init.normal_(self.v, 0., 0.02)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"Reset the internal memory.\"\n",
    "        self.hidden = [next(self.parameters()).data.new(0) for i in range(self.n_layers+1)]\n",
    "\n",
    "    def _update_mems(self, hids):\n",
    "        if not getattr(self, 'hidden', False): return None\n",
    "        assert len(hids) == len(self.hidden), 'len(hids) != len(self.hidden)'\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(hids)):\n",
    "                cat = torch.cat([self.hidden[i], hids[i]], dim=1)\n",
    "                self.hidden[i] = cat[:,-self.mem_len:].detach()\n",
    "    \n",
    "    def select_hidden(self, idxs): self.hidden = [h[idxs] for h in self.hidden]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #The hidden state has to be initiliazed in the forward pass for nn.DataParallel\n",
    "        if self.mem_len > 0 and not self.init: \n",
    "            self.reset()\n",
    "            self.init = True\n",
    "        bs,x_len,emb_sz = x.size()\n",
    "        \n",
    "        inp = x\n",
    "        \n",
    "        m_len = self.hidden[0].size(1) if hasattr(self, 'hidden') and len(self.hidden[0].size()) > 1 else 0\n",
    "        seq_len = m_len + x_len\n",
    "        mask = torch.triu(x.new_ones(x_len, seq_len).long(), diagonal=1+m_len).byte()[None,None] if self.mask else None\n",
    "        \n",
    "        hids = []\n",
    "        pos = torch.arange(seq_len-1, -1, -1, device=inp.device, dtype=inp.dtype)\n",
    "        pos_enc = self.pos_enc(pos)\n",
    "        hids.append(inp)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            mem = self.hidden[i] if self.mem_len > 0 else None\n",
    "            inp = layer(inp, r=pos_enc, u=self.u, v=self.v, mask=mask, mem=mem)\n",
    "            hids.append(inp)\n",
    "        core_out = inp[:,-x_len:]\n",
    "        if self.mem_len > 0 : self._update_mems(hids)\n",
    "        return core_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainType = Enum('TrainType', 'Default, Separate, LMOnly, BertOnly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertHead(nn.Module):\n",
    "    def __init__(self, embed, bert_encoder, nw_encoder, decoder, train_type=TrainType.Default):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "        self.bert_encoder = bert_encoder\n",
    "        self.nw_encoder = nw_encoder\n",
    "        self.decoder = decoder\n",
    "        self.train_type = train_type\n",
    "        \n",
    "    \n",
    "    def forward(self, x, mask_idxs=None):\n",
    "        x_enc = self.embed(x)\n",
    "        self.bert_encoder.mask = False\n",
    "        \n",
    "        # Baseline\n",
    "        if self.train_type == TrainType.LMOnly:\n",
    "            nw_enc = self.nw_encoder(x_enc)\n",
    "            return self.decoder(nw_enc)\n",
    "        \n",
    "        if self.train_type == TrainType.BertOnly:\n",
    "            bert_enc = self.bert_encoder(x_enc)\n",
    "            return self.decoder(bert_enc)\n",
    "        \n",
    "        # Validation - train separately\n",
    "        if (self.train_type == TrainType.Separate) or (not self.training):\n",
    "            bert_enc = self.bert_encoder(x_enc)\n",
    "            \n",
    "            x_lm_enc = self.embed(mask_idxs)\n",
    "            nw_enc = self.nw_encoder(x_lm_enc)\n",
    "            return self.decoder(bert_enc), self.decoder(nw_enc)\n",
    "        \n",
    "        bert_first = mask_idxs is None # mask idxs tells us which embeddings to mask\n",
    "        if bert_first:\n",
    "            self.bert_encoder.mask = True\n",
    "            bert_enc = self.bert_encoder(x_enc)\n",
    "            nw_enc = self.nw_encoder(bert_enc)\n",
    "            return self.decoder(bert_enc), self.decoder(nw_enc)\n",
    "        else:\n",
    "            nw_enc = self.nw_encoder(x_enc)\n",
    "            nw_enc[mask_idxs] = embed(torch.tensor(mask_idx, device=x.device))\n",
    "            bert_enc = self.bert_encoder(nw_enc)\n",
    "            return self.decoder(bert_enc), self.decoder(nw_enc)\n",
    "    \n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int, n_hid:int, output_p:float, tie_encoder:nn.Module=None, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        output = self.output_dp(input)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sz = len(data.vocab.itos)\n",
    "# config = tfmer_lm_config.copy(); config\n",
    "config = {\n",
    "    'ctx_len': bptt,\n",
    "    'n_layers': 4,\n",
    "    'n_heads': 4,\n",
    "    'd_model': 128,\n",
    "    'd_head': 32,\n",
    "    'd_inner': 512,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLoss():\n",
    "    def __init__(self, pad_idx=pad_idx, loss_mult=(1,1)):\n",
    "        \"Loss mult - mask, NextWord, Seq2Seq, NextSent\"\n",
    "        self.index_loss = CrossEntropyFlat(ignore_index=pad_idx)\n",
    "        self.loss_mult = loss_mult\n",
    "        \n",
    "    def __call__(self, input:Tensor, bert_target:Tensor, lm_target:Tensor, **kwargs)->Rank0Tensor:\n",
    "        x_bert, x_lm = input\n",
    "        loss_bert = self.index_loss.__call__(x_bert, bert_target, **kwargs) * self.loss_mult[0]\n",
    "        loss_lm = self.index_loss.__call__(x_lm, lm_target, **kwargs) * self.loss_mult[1]\n",
    "#         print(loss_bert, loss_lm)\n",
    "        return loss_bert + loss_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc_ignore_pad(input:Tensor, targ:Tensor, pad_idx=pad_idx)->Rank0Tensor:\n",
    "    n = targ.shape[0]\n",
    "    input = input.argmax(dim=-1).view(n,-1)\n",
    "    targ = targ.view(n,-1)\n",
    "    mask = targ != pad_idx\n",
    "    return (input[mask]==targ[mask]).float().mean()\n",
    "\n",
    "def bert_acc(input:Tensor, b_t:Tensor, lm_t:Tensor)->Rank0Tensor:\n",
    "    x_bert, x_lm = input\n",
    "    return acc_ignore_pad(x_bert, b_t)\n",
    "def lm_acc(input:Tensor, b_t:Tensor, lm_t:Tensor)->Rank0Tensor:\n",
    "    x_bert, x_lm = input\n",
    "    return acc_ignore_pad(x_lm, lm_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_dl.tfms = [rnd_tfm]\n",
    "data.valid_dl.tfms = [val_tfm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedder(vocab_sz, config['d_model'])\n",
    "bert_encoder = Encoder(**config)\n",
    "nw_encoder = Encoder(**config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=embed.embed, bias=False, output_p=False)\n",
    "bert_head = BertHead(embed, bert_encoder, nw_encoder, decoder)\n",
    "model = bert_head\n",
    "model.apply(init_transformer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, model, loss_func=BertLoss(), clip=0.5)\n",
    "learn.callbacks = []\n",
    "learn.metrics=[bert_acc, lm_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics=[bert_acc, lm_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(num_it=500)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>lm_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12.452651</td>\n",
       "      <td>11.264405</td>\n",
       "      <td>0.232077</td>\n",
       "      <td>0.218978</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.469805</td>\n",
       "      <td>10.642893</td>\n",
       "      <td>0.271561</td>\n",
       "      <td>0.241542</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Train Separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_dl.tfms = [val_tfm]\n",
    "data.valid_dl.tfms = [val_tfm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedder(vocab_sz, config['d_model'])\n",
    "bert_encoder = Encoder(**config)\n",
    "nw_encoder = Encoder(**config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=embed.embed, bias=False, output_p=False)\n",
    "bert_head = BertHead(embed, bert_encoder, nw_encoder, decoder, train_type=TrainType.Separate)\n",
    "model = bert_head\n",
    "model.apply(init_transformer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, model, loss_func=BertLoss(), clip=0.5)\n",
    "learn.callbacks = []\n",
    "learn.metrics=[bert_acc, lm_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>lm_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.521917</td>\n",
       "      <td>10.406860</td>\n",
       "      <td>0.314711</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.978490</td>\n",
       "      <td>9.652961</td>\n",
       "      <td>0.363171</td>\n",
       "      <td>0.266574</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Next Word only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedder(vocab_sz, config['d_model'])\n",
    "nw_encoder = Encoder(**config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=embed.embed, bias=False, output_p=False)\n",
    "model = nn.Sequential(embed, nw_encoder, decoder)\n",
    "model.apply(init_transformer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_dl.tfms = []\n",
    "data.valid_dl.tfms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.386352</td>\n",
       "      <td>5.950587</td>\n",
       "      <td>0.146343</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.591476</td>\n",
       "      <td>5.380808</td>\n",
       "      <td>0.235560</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Bert only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedder(vocab_sz, config['d_model'])\n",
    "bert_encoder = Encoder(**config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=embed.embed, bias=False, output_p=False)\n",
    "model = nn.Sequential(embed, bert_encoder, decoder)\n",
    "model.apply(init_transformer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = CrossEntropyFlat(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, model, loss_func=loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tfm(b):\n",
    "    x_msk, (y_msk, y_lm) = bert_first_tfm(b)\n",
    "    return x_msk, y_msk\n",
    "\n",
    "data.train_dl.tfms = [bert_tfm]\n",
    "data.valid_dl.tfms = [bert_tfm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_acc(input:Tensor, t1:Tensor)->Rank0Tensor:\n",
    "    return acc_ignore_pad(input, t1)\n",
    "learn.metrics=[base_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>base_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.851985</td>\n",
       "      <td>5.431993</td>\n",
       "      <td>0.246056</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.378727</td>\n",
       "      <td>5.213453</td>\n",
       "      <td>0.268824</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
