{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/fastai/fastai/blob/097a942622a4fe172919a7126bacc207fdda0eb6/examples/train_wt103.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##To train a language model on Wikitext-103\n",
    "##`python train_wt103.py fwd` for the forward pretrained model in fastai\n",
    "##`python train_wt103.py bwd --backwards True` for the backward pretrained model in fastai\n",
    "## Takes 6 hours on a Titan RTX (24Gb RAM), adjust batch size and lr if less GPU RAM\n",
    "\n",
    "from fastai.text import *\n",
    "from fastai.script import *\n",
    "from fastprogress import fastprogress\n",
    "\n",
    "#Functions to parse WT103 in separate articles\n",
    "def istitle(line):\n",
    "    return len(re.findall(r'^ = [^=]* = $', line)) != 0\n",
    "\n",
    "def read_file(filename):\n",
    "    articles = []\n",
    "    with open(filename, encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "    current_article = ''\n",
    "    for i,line in enumerate(lines):\n",
    "        current_article += line\n",
    "        if i < len(lines)-2 and lines[i+1] == ' \\n' and istitle(lines[i+2]):\n",
    "            current_article = current_article.replace('<unk>', UNK)\n",
    "            articles.append(current_article)\n",
    "            current_article = ''\n",
    "    current_article = current_article.replace('<unk>', UNK)\n",
    "    articles.append(current_article)\n",
    "    return np.array(articles)\n",
    "\n",
    "def create_data(path):\n",
    "    train = read_file(path/'train.csv')\n",
    "    valid = read_file(path/'test.csv')\n",
    "#     test =  read_file(path/'test.txt')\n",
    "    test = None\n",
    "    all_texts = np.concatenate([valid, train])\n",
    "    df = pd.DataFrame({'texts':all_texts})\n",
    "    del train ; del valid ; del text #Free RQM before tokenizing\n",
    "    data = (TextList.from_df(df, path, cols='texts')\n",
    "                    .split_by_idx(range(0,60))\n",
    "                    .label_for_lm()\n",
    "                    .databunch())\n",
    "    data.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.WIKITEXT_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = untar_data(URLs.WIKITEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ubuntu/.fastai/data/wikitext-2')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'text' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b083531a9aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# path = Config().data_path()/'wikitext-2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# fastprogress.SAVE_PATH = f'{name}.txt' #Save the output of the progress bar in {name}.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'data_save.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcreate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-4f6f5dbc47aa>\u001b[0m in \u001b[0;36mcreate_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mall_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'texts'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mall_texts\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mdel\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;31m#Free RQM before tokenizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     data = (TextList.from_df(df, path, cols='texts')\n\u001b[1;32m     38\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0msplit_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'text' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\"Training on Wikitext 103\"\n",
    "# path = Config().data_path()/'wikitext-103'\n",
    "# path = Config().data_path()/'wikitext-2'\n",
    "# fastprogress.SAVE_PATH = f'{name}.txt' #Save the output of the progress bar in {name}.txt\n",
    "if not (path/'data_save.pkl').is_file(): create_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@call_parse\n",
    "def main(\n",
    "        name:Param(\"Name of the experiment\", str, opt=False),\n",
    "        gpu:Param(\"GPU to run on\", int)=0,\n",
    "        lr: Param(\"Learning rate\", float)=1e-2,\n",
    "        drop_mult: Param(\"Dropouts multiplicator\", float)=0.1,\n",
    "        wd: Param(\"Weight Decay\", float)=0.1,\n",
    "        epochs: Param(\"Number of epochs\", int)=12,\n",
    "        bs: Param(\"Batch size\", int)=256,\n",
    "        bptt: Param(\"Bptt\", int)=80,\n",
    "        backwards: Param(\"Backward model\", bool)=False\n",
    "        ):\n",
    "    \"Training on Wikitext 103\"\n",
    "    path = Config().data_path()/'wikitext-103'\n",
    "    fastprogress.SAVE_PATH = f'{name}.txt' #Save the output of the progress bar in {name}.txt\n",
    "    torch.cuda.set_device(gpu)\n",
    "    if not (path/'data_save.pkl').is_file(): create_data(path)\n",
    "    data = load_data(path, bs=bs, bptt=bptt, backwards=backwards)\n",
    "    learn = language_model_learner(data, AWD_LSTM, drop_mult=drop_mult, pretrained=False,\n",
    "                                   metrics=[accuracy, Perplexity()])\n",
    "    learn = learn.to_fp16(clip=0.1)\n",
    "\n",
    "    learn.fit_one_cycle(epochs, lr, moms=(0.8,0.7), div_factor=10, wd=wd)\n",
    "\n",
    "    learn = learn.to_fp32()\n",
    "    learn.save(f'{name}', with_opt=False)\n",
    "    learn.data.vocab.save(path/f'{name}_vocab.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
