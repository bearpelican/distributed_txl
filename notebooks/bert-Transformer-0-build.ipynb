{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from fastai import *        # Quick accesss to most common functionality\n",
    "# from fastai.text import *   # Quick accesss to NLP functionality\n",
    "import html\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.transformer import tfmer_lm_config, GeLU, init_transformer\n",
    "from fastai.text.models.awd_lstm import RNNDropout\n",
    "from fastai.text.learner import LanguageLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path.home()/'data/wikitext-2-raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basic_data import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert task - dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 256\n",
    "data = load_data(PATH, bs=8, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = 'xxmask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = data.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.itos.append(MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>team of writers handled the script . xxmaj the game 's opening theme was sung by xxmaj may ' n . xxbos xxmaj it met with positive sales in xxmaj japan , and was praised by both xxmaj japanese and western critics . xxmaj after release , it received downloadable content , along with an expanded edition in xxmaj november of that year . xxmaj it was also adapted into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>, which relentlessly and brutally and xxunk keeps these vicious , murderous wars . xxmaj it is a vandal state . xxmaj there is a xxmaj russian writer who once described vandal states as xxmaj genghis xxmaj khan with a telegraph . xxmaj israel is xxmaj genghis xxmaj khan with a computer . i feel no emotion of affinity with that state . i have some good friends and their</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>the 5th xxmaj division to envelop xxup un troops and push them back to xxmaj pusan . xxmaj the 766th was not reinforced ; xxmaj north xxmaj korean planners intended it to move unseen around the xxup un lines while the majority of the xxup un and xxmaj north xxmaj korean troops were locked in fighting around xxmaj taegu and the xxmaj xxunk xxmaj bulge . xxbos xxmaj by this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>at least two \" facilitating subjects \" ; the average point score per qualification was xxunk , equating to a xxup c- grade , and the average point score per student was xxunk . xxmaj the xxmaj sunday xxmaj times ranked xxmaj carre 's 101st ( 49th amongst state schools ) in the xxmaj midlands and 750th nationally based on a - xxmaj level and xxup gcse performance in 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>mfume , the president of the xxmaj national xxmaj association for the xxmaj advancement of xxmaj colored xxmaj people ( xxup naacp ) , would run . xxmaj mfume had previously served on the xxmaj baltimore xxmaj city xxmaj council and in the xxmaj united xxmaj states xxmaj house of xxmaj representatives . xxmaj schmoke called the race \" his to lose \" . xxmaj however , xxmaj mfume lived</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    9,  3531,  5611,  ...,  1105,   622,    29],\n",
       "         [16077,    19,  4075,  ..., 17572,    60,     9],\n",
       "         [   12,     9,     6,  ...,   344,    12,     5],\n",
       "         ...,\n",
       "         [    9, 13634,   749,  ...,    54,    16,     5],\n",
       "         [ 1228,   367,   785,  ...,    16,  2218,    12],\n",
       "         [   15,  4549,     9,  ...,    21,  1107,   869]]),\n",
       " tensor([[ 3531,  5611,  3398,  ...,   622,    29,     6],\n",
       "         [   19,  4075,    16,  ...,    60,     9,   465],\n",
       "         [    9,     6,  6630,  ...,    12,     5,   113],\n",
       "         ...,\n",
       "         [13634,   749,    23,  ...,    16,     5,   307],\n",
       "         [  367,   785,   321,  ...,  2218,    12,  1636],\n",
       "         [ 4549,     9, 17780,  ...,  1107,   869,    11]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_range = (0, len(data.vocab.itos))\n",
    "from fastai.text.transform import *\n",
    "pad_idx = data.vocab.stoi[PAD]\n",
    "mask_idx = data.vocab.stoi[MASK]\n",
    "def mask_tfm(b, word_range=word_range, pad_idx=pad_idx, \n",
    "             mask_idx=mask_idx, p=0.2):\n",
    "    # p = replacement probability\n",
    "    x,y_lm = b\n",
    "    x,y_msk = x.clone(),x.clone() # x, x\n",
    "#     x,y = x.clone(),y.clone()\n",
    "    rand = torch.rand(x.shape, device=x.device)\n",
    "    y_msk[rand > p] = pad_idx\n",
    "    x[rand <= (p*.8)] = mask_idx # 80% = mask\n",
    "    wrong_word = (rand > (p*.8)) & (rand <= (p*.9)) # 10% = wrong word\n",
    "    x[wrong_word] = torch.randint(*word_range, [wrong_word.sum().item()], device=x.device)\n",
    "    return x, (y_msk, y_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nw_tfm(b, pad_idx=pad_idx, \n",
    "             mask_idx=mask_idx, p=0.2):\n",
    "    # p = replacement probability\n",
    "    x,y_lm = b\n",
    "    \n",
    "    y_msk = y_lm.clone() # x, x\n",
    "    rand = torch.rand(x.shape, device=x.device)\n",
    "    y_msk[rand > p] = pad_idx\n",
    "    x_msk = rand <= p\n",
    "    return (x, x_msk), (y_msk, y_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.train_dl.add_tfm(mask_tfm)\n",
    "# data.valid_dl.add_tfm(mask_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_dl.add_tfm(nw_tfm)\n",
    "data.valid_dl.add_tfm(nw_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_language_model(TransformerXL, len(data.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_sz = len(data.vocab.itos)\n",
    "# # config = tfmer_lm_config.copy(); config\n",
    "# config = {\n",
    "#     'ctx_len': bptt,\n",
    "#     'n_layers': 4,\n",
    "#     'n_heads': 4,\n",
    "#     'd_model': 128,\n",
    "#     'd_head': 32,\n",
    "#     'd_inner': 512,\n",
    "# }\n",
    "\n",
    "# embed = Embedder(vocab_sz, config['d_model'])\n",
    "# bert_encoder = Encoder(**config)\n",
    "# nw_encoder = Encoder(**config)\n",
    "# decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=embed.embed, bias=False, output_p=False)\n",
    "# bert_head = BertHead(embed, bert_encoder, nw_encoder, decoder)\n",
    "# model = bert_head\n",
    "# model.apply(init_transformer);\n",
    "\n",
    "# embed(torch.tensor(mask_idx))\n",
    "\n",
    "# e_out = embed(xb[0])\n",
    "# nw_out = nw_encoder(e_out)\n",
    "\n",
    "# nw_out.shape\n",
    "\n",
    "# xb[1].shape\n",
    "\n",
    "# xb[1][0][0] = 1\n",
    "\n",
    "# nw_out[xb[1]] = embed(torch.tensor(mask_idx, device=xb[0].device))\n",
    "\n",
    "# nw_out\n",
    "\n",
    "# xb[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder(nn.Module):\n",
    "    \"Embedding + positional encoding + dropout\"\n",
    "    def __init__(self, vocab_sz:int, emb_sz:int, embed_p:float=0., pad_idx=pad_idx):\n",
    "        super().__init__()\n",
    "        self.emb_sz = emb_sz\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_idx)\n",
    "        # See https://arxiv.org/abs/1711.09160\n",
    "        with torch.no_grad(): trunc_normal_(self.embed.weight, std=0.01)\n",
    "        self.drop = nn.Dropout(embed_p)\n",
    "    \n",
    "    def forward(self, inp, pos_forward=False):\n",
    "        emb = self.drop(self.embed(inp))\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"TransformerXL model: https://arxiv.org/abs/1901.02860.\"\n",
    "    def __init__(self, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., bias:bool=False, scale:bool=True,\n",
    "                 act:Activation=Activation.ReLU, double_drop:bool=True, attn_cls:Callable=MultiHeadRelativeAttention,\n",
    "                 learned_pos_enc:bool=False, mask:bool=True, mem_len:int=0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.u = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.v = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.mem_len,self.n_layers,self.d_model,self.mask = mem_len,n_layers,d_model,mask\n",
    "        self.init = False\n",
    "        self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "                      ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "                      attn_cls=attn_cls) for k in range(n_layers)])\n",
    "        \n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"Reset the internal memory.\"\n",
    "        self.hidden = [next(self.parameters()).data.new(0) for i in range(self.n_layers+1)]\n",
    "\n",
    "    def _update_mems(self, hids):\n",
    "        if not getattr(self, 'hidden', False): return None\n",
    "        assert len(hids) == len(self.hidden), 'len(hids) != len(self.hidden)'\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(hids)):\n",
    "                cat = torch.cat([self.hidden[i], hids[i]], dim=1)\n",
    "                self.hidden[i] = cat[:,-self.mem_len:].detach()\n",
    "    \n",
    "    def select_hidden(self, idxs): self.hidden = [h[idxs] for h in self.hidden]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #The hidden state has to be initiliazed in the forward pass for nn.DataParallel\n",
    "        if self.mem_len > 0 and not self.init: \n",
    "            self.reset()\n",
    "            self.init = True\n",
    "        bs,x_len,emb_sz = x.size()\n",
    "        \n",
    "        inp = x\n",
    "        \n",
    "        m_len = self.hidden[0].size(1) if hasattr(self, 'hidden') and len(self.hidden[0].size()) > 1 else 0\n",
    "        seq_len = m_len + x_len\n",
    "        mask = torch.triu(x.new_ones(x_len, seq_len).long(), diagonal=1+m_len).byte()[None,None] if self.mask else None\n",
    "        \n",
    "        hids = []\n",
    "        pos = torch.arange(seq_len-1, -1, -1, device=inp.device, dtype=inp.dtype)\n",
    "        pos_enc = self.pos_enc(pos)\n",
    "        hids.append(inp)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            mem = self.hidden[i] if self.mem_len > 0 else None\n",
    "            inp = layer(inp, r=pos_enc, u=self.u, v=self.v, mask=mask, mem=mem)\n",
    "            hids.append(inp)\n",
    "        core_out = inp[:,-x_len:]\n",
    "        if self.mem_len > 0 : self._update_mems(hids)\n",
    "        return core_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertHead(nn.Module):\n",
    "    def __init__(self, embed, bert_encoder, nw_encoder, decoder, lm_only=False):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "        self.bert_encoder = bert_encoder\n",
    "        self.nw_encoder = nw_encoder\n",
    "        self.decoder = decoder\n",
    "        self.lm_only = lm_only\n",
    "        \n",
    "    \n",
    "    def forward(self, x, mask_idxs=None):\n",
    "        \n",
    "        x_enc = self.embed(x)\n",
    "#         x_enc, _, _ = self.embed(x)\n",
    "        if self.lm_only:\n",
    "            nw_enc = self.nw_encoder(x_enc)\n",
    "            return self.decoder(nw_enc)\n",
    "        bert_first = mask_idxs is None # mask idxs tells us which embeddings to mask\n",
    "        if bert_first:\n",
    "            bert_enc = self.bert_encoder(x_enc)\n",
    "            nw_enc = self.nw_encoder(bert_enc)\n",
    "            return self.decoder(bert_enc), self.decoder(nw_enc)\n",
    "        else:\n",
    "            nw_enc = self.nw_encoder(x_enc)\n",
    "            nw_enc[mask_idxs] = embed(torch.tensor(mask_idx, device=x.device))\n",
    "            bert_enc = self.bert_encoder(nw_enc)\n",
    "            return self.decoder(bert_enc), self.decoder(nw_enc)\n",
    "    \n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int, n_hid:int, output_p:float, tie_encoder:nn.Module=None, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        output = self.output_dp(input)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sz = len(data.vocab.itos)\n",
    "# config = tfmer_lm_config.copy(); config\n",
    "config = {\n",
    "    'ctx_len': bptt,\n",
    "    'n_layers': 4,\n",
    "    'n_heads': 4,\n",
    "    'd_model': 128,\n",
    "    'd_head': 32,\n",
    "    'd_inner': 512,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLoss():\n",
    "    def __init__(self, pad_idx=pad_idx):\n",
    "        \"Loss mult - mask, NextWord, Seq2Seq, NextSent\"\n",
    "        self.index_loss = CrossEntropyFlat(ignore_index=pad_idx)\n",
    "        \n",
    "    def __call__(self, input:Tensor, lm_target:Tensor, bert_target:Tensor, **kwargs)->Rank0Tensor:\n",
    "        x_bert, x_lm = input\n",
    "        loss_bert = self.index_loss.__call__(x_bert, bert_target, **kwargs)\n",
    "        loss_lm = self.index_loss.__call__(x_lm, lm_target, **kwargs)\n",
    "#         print(loss_bert, loss_lm)\n",
    "        return loss_bert + loss_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BertLearner(LanguageLearner):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc_ignore_pad(input:Tensor, targ:Tensor, pad_idx=pad_idx)->Rank0Tensor:\n",
    "    n = targ.shape[0]\n",
    "    input = input.argmax(dim=-1).view(n,-1)\n",
    "    targ = targ.view(n,-1)\n",
    "    mask = targ != pad_idx\n",
    "    return (input[mask]==targ[mask]).float().mean()\n",
    "\n",
    "def bert_acc(input:Tensor, t1:Tensor, t2:Tensor)->Rank0Tensor:\n",
    "    return acc_ignore_pad(input[0], t1)\n",
    "def lm_acc(input:Tensor, t1:Tensor, t2:Tensor)->Rank0Tensor:\n",
    "    return acc_ignore_pad(input[1], t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedder(vocab_sz, config['d_model'])\n",
    "bert_encoder = Encoder(**config)\n",
    "nw_encoder = Encoder(**config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=embed.embed, bias=False, output_p=False)\n",
    "bert_head = BertHead(embed, bert_encoder, nw_encoder, decoder)\n",
    "model = bert_head\n",
    "model.apply(init_transformer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, model, loss_func=BertLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics=[bert_acc, lm_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch(cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(*xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 39881])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.1966, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.loss_func(out, *yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21.195696, tensor(0.0070), tensor(0.0342)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-4.9580e-01, -9.6619e-02, -5.2701e-01,  ..., -4.7588e-02,\n",
       "           -1.3230e-01,  3.7609e-01],\n",
       "          [ 1.5931e+00, -4.1648e-02,  2.2567e-01,  ..., -1.5223e-01,\n",
       "           -2.6749e-01,  1.7577e-02],\n",
       "          [ 1.8046e-01,  1.4072e-01,  1.5947e-03,  ..., -3.6805e-01,\n",
       "            2.7003e-01,  2.6057e-02],\n",
       "          ...,\n",
       "          [ 7.7676e-01, -9.8936e-02,  4.2904e-01,  ..., -1.6872e-01,\n",
       "           -3.4325e-01,  1.0617e-01],\n",
       "          [ 2.9330e-01,  8.7012e-02,  4.0908e-01,  ..., -1.4277e-01,\n",
       "           -6.7441e-02,  7.8620e-02],\n",
       "          [-3.1922e-02,  2.7619e-01,  4.4806e-02,  ..., -2.3188e-01,\n",
       "            7.5024e-01, -1.3021e-01]],\n",
       " \n",
       "         [[ 1.5724e-01,  2.2869e-01, -3.8199e-01,  ..., -2.6260e-01,\n",
       "           -3.5921e-01,  5.0073e-01],\n",
       "          [-6.6347e-03,  1.1319e-01,  1.2132e-02,  ..., -1.6496e-01,\n",
       "           -3.0566e-01,  5.1331e-02],\n",
       "          [-1.1949e-01,  1.8414e-01, -2.1055e-02,  ..., -2.6595e-01,\n",
       "            7.7257e-01, -8.9324e-02],\n",
       "          ...,\n",
       "          [ 9.4122e-02, -2.5750e-02, -1.8331e-01,  ..., -2.6548e-02,\n",
       "            7.1864e-02, -9.6961e-02],\n",
       "          [-1.8547e-01, -2.4669e-01, -2.8619e-01,  ...,  7.0873e-02,\n",
       "           -7.0686e-02, -3.6993e-01],\n",
       "          [ 1.6966e-01,  1.0910e-01,  1.3344e-02,  ..., -3.5702e-01,\n",
       "            2.3272e-01, -3.8384e-02]],\n",
       " \n",
       "         [[ 1.8825e-01,  3.4233e-01,  2.2667e-01,  ..., -3.7931e-01,\n",
       "           -2.9405e-01, -5.0189e-02],\n",
       "          [-7.3704e-02,  2.2203e-01,  6.2610e-03,  ..., -2.2055e-01,\n",
       "            7.3138e-01, -1.1592e-01],\n",
       "          [ 1.2483e+00, -1.8524e-01,  2.3247e-01,  ...,  1.2717e-02,\n",
       "            2.7766e-01, -9.6344e-02],\n",
       "          ...,\n",
       "          [-2.0331e-01, -2.6010e-01,  4.6107e-01,  ...,  3.4597e-01,\n",
       "           -4.0145e-02, -6.4674e-01],\n",
       "          [ 1.4894e-01,  7.4526e-02,  1.0391e-01,  ..., -3.1543e-01,\n",
       "           -4.4363e-01,  1.4077e-01],\n",
       "          [ 4.4112e-02,  3.0826e-02, -1.2227e-01,  ...,  2.7343e-01,\n",
       "           -1.2003e-02, -1.0821e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 2.6665e-01,  4.6505e-02, -3.2050e-01,  ..., -3.8086e-01,\n",
       "           -9.8873e-02, -1.4655e-01],\n",
       "          [ 4.8524e-01, -7.0896e-02,  5.4742e-02,  ...,  1.9167e-01,\n",
       "           -2.5246e-01,  3.1030e-01],\n",
       "          [-1.6107e-01,  9.0862e-02, -1.6939e-01,  ..., -1.0389e-01,\n",
       "           -2.2287e-01,  7.2677e-02],\n",
       "          ...,\n",
       "          [ 2.2405e-01,  1.5965e-01, -1.4262e-01,  ..., -1.6854e-01,\n",
       "            1.1964e-02, -4.4519e-02],\n",
       "          [-2.1416e-01, -1.0141e-01, -3.9890e-01,  ...,  3.7324e-01,\n",
       "           -5.3288e-02,  2.0818e-01],\n",
       "          [ 5.3292e-02,  1.4576e-01, -1.3148e-03,  ..., -1.1936e-01,\n",
       "           -2.7765e-01,  7.9081e-02]],\n",
       " \n",
       "         [[-1.0304e-01,  2.1579e-01, -3.6351e-03,  ..., -2.2315e-01,\n",
       "            7.0067e-01, -4.5397e-02],\n",
       "          [ 3.7120e-01,  6.8140e-02,  4.4323e-03,  ..., -1.8667e-01,\n",
       "           -2.9881e-01, -4.0342e-01],\n",
       "          [-1.9274e-02, -2.6049e-02,  2.3646e-01,  ...,  7.5650e-02,\n",
       "           -4.2175e-01, -4.2401e-02],\n",
       "          ...,\n",
       "          [-5.4272e-02, -1.0210e-02, -1.1597e-01,  ...,  3.1657e-01,\n",
       "           -3.9264e-01, -1.3385e-01],\n",
       "          [-2.5581e-02, -2.2870e-01,  8.8557e-02,  ..., -4.7068e-01,\n",
       "            5.2649e-01, -2.1066e-01],\n",
       "          [ 4.5261e-01,  1.2364e-01,  4.2352e-01,  ..., -3.7767e-01,\n",
       "           -2.9663e-01, -2.0880e-01]],\n",
       " \n",
       "         [[ 2.2408e-01,  6.6718e-02,  2.3682e-01,  ...,  2.7474e-01,\n",
       "           -3.1502e-01,  1.8510e-01],\n",
       "          [ 2.4650e-02, -2.7724e-03, -3.2820e-02,  ..., -8.4715e-02,\n",
       "            3.4134e-02, -3.2672e-01],\n",
       "          [ 2.3041e-01,  5.1259e-02, -3.6303e-01,  ..., -3.8755e-01,\n",
       "           -7.0734e-02, -1.7374e-01],\n",
       "          ...,\n",
       "          [ 2.1099e-01, -4.0009e-01, -1.9961e-02,  ...,  1.2281e-01,\n",
       "            3.3811e-01, -1.7605e-01],\n",
       "          [-5.8160e-02,  1.9748e-01,  4.4859e-02,  ..., -2.5131e-01,\n",
       "            6.9369e-01, -4.4179e-02],\n",
       "          [-1.0380e-01,  5.4658e-01,  3.3710e-01,  ...,  2.7812e-01,\n",
       "            2.3282e-01,  2.2838e-01]]]),\n",
       " tensor([[[-4.8551e-01, -1.5682e-01, -5.6143e-01,  ..., -9.0555e-02,\n",
       "           -1.4102e-01,  4.1990e-01],\n",
       "          [ 4.9085e-02,  2.4296e-04,  1.0585e-02,  ..., -5.0847e-03,\n",
       "           -5.3023e-04,  1.5823e-03],\n",
       "          [ 1.2638e-01,  1.5531e-01,  5.3369e-03,  ..., -3.3293e-01,\n",
       "            3.2440e-01, -3.5481e-02],\n",
       "          ...,\n",
       "          [ 4.9085e-02,  2.4296e-04,  1.0585e-02,  ..., -5.0847e-03,\n",
       "           -5.3023e-04,  1.5823e-03],\n",
       "          [ 2.8371e-01,  9.7506e-02,  3.9446e-01,  ..., -2.0700e-01,\n",
       "           -8.2753e-02,  5.7515e-02],\n",
       "          [ 3.8813e-03,  2.3539e-01,  3.9144e-02,  ..., -3.0850e-01,\n",
       "            8.1616e-01, -8.5297e-02]],\n",
       " \n",
       "         [[ 9.6852e-02,  2.3310e-01, -3.5598e-01,  ..., -2.4103e-01,\n",
       "           -2.9436e-01,  5.1532e-01],\n",
       "          [-5.6023e-02,  1.1590e-01,  8.4872e-02,  ..., -1.7787e-01,\n",
       "           -3.1044e-01,  2.7135e-02],\n",
       "          [-4.8711e-02,  2.1125e-01,  1.6704e-02,  ..., -3.3570e-01,\n",
       "            8.4015e-01, -6.1484e-02],\n",
       "          ...,\n",
       "          [ 1.1301e-01, -8.0701e-02, -1.5415e-01,  ..., -3.5912e-02,\n",
       "            1.4639e-01, -1.4534e-01],\n",
       "          [-2.0927e-01, -2.1200e-01, -2.0434e-01,  ...,  1.0858e-01,\n",
       "           -7.8082e-02, -3.3504e-01],\n",
       "          [ 1.4142e-01,  1.1202e-01,  3.3230e-02,  ..., -3.4770e-01,\n",
       "            3.2085e-01, -5.3828e-02]],\n",
       " \n",
       "         [[ 1.8334e-01,  3.5069e-01,  2.3965e-01,  ..., -3.1103e-01,\n",
       "           -2.7351e-01, -9.4916e-02],\n",
       "          [-1.4906e-02,  2.3592e-01,  4.1735e-02,  ..., -3.1143e-01,\n",
       "            8.3959e-01, -9.3259e-02],\n",
       "          [ 4.9085e-02,  2.4296e-04,  1.0585e-02,  ..., -5.0847e-03,\n",
       "           -5.3023e-04,  1.5823e-03],\n",
       "          ...,\n",
       "          [-1.8373e-01, -2.0847e-01,  5.3280e-01,  ...,  3.9743e-01,\n",
       "           -3.4009e-02, -6.2357e-01],\n",
       "          [ 2.1502e-01,  6.5871e-02,  1.9153e-01,  ..., -3.7974e-01,\n",
       "           -4.5178e-01,  1.6141e-01],\n",
       "          [ 6.5248e-02,  7.6476e-02, -4.5498e-02,  ...,  2.4263e-01,\n",
       "            5.1431e-02, -3.7616e-02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 2.1298e-01,  8.7556e-02, -2.9572e-01,  ..., -3.1427e-01,\n",
       "           -6.3788e-02, -1.0343e-01],\n",
       "          [ 4.2939e-01, -2.4220e-02,  4.9653e-02,  ...,  1.8691e-01,\n",
       "           -2.3685e-01,  2.9614e-01],\n",
       "          [-1.7649e-01,  1.8347e-01, -2.0344e-01,  ..., -1.4786e-01,\n",
       "           -1.5903e-01,  5.2901e-02],\n",
       "          ...,\n",
       "          [ 2.4563e-01,  1.8176e-01, -5.1349e-02,  ..., -2.3912e-01,\n",
       "            1.4752e-02, -5.5942e-02],\n",
       "          [-1.7818e-01, -7.9703e-02, -3.3835e-01,  ...,  4.1701e-01,\n",
       "           -3.3179e-03,  1.7444e-01],\n",
       "          [ 2.5076e-02,  1.7295e-01,  9.7552e-02,  ..., -1.6486e-01,\n",
       "           -3.0206e-01,  6.9962e-02]],\n",
       " \n",
       "         [[-4.0389e-02,  2.3816e-01,  2.5194e-02,  ..., -3.0262e-01,\n",
       "            7.8633e-01, -4.1128e-02],\n",
       "          [ 3.6942e-01,  9.8242e-02,  6.1763e-02,  ..., -1.5876e-01,\n",
       "           -3.5571e-01, -4.4102e-01],\n",
       "          [ 1.6005e-02, -2.1232e-02,  2.2553e-01,  ...,  7.4182e-02,\n",
       "           -4.9119e-01, -1.0194e-01],\n",
       "          ...,\n",
       "          [-4.3463e-02,  7.7964e-03, -5.3599e-02,  ...,  3.4550e-01,\n",
       "           -3.7038e-01, -1.0918e-01],\n",
       "          [-5.8276e-02, -1.6689e-01,  1.0112e-01,  ..., -4.4439e-01,\n",
       "            5.0644e-01, -2.2131e-01],\n",
       "          [ 4.9085e-02,  2.4296e-04,  1.0585e-02,  ..., -5.0847e-03,\n",
       "           -5.3023e-04,  1.5823e-03]],\n",
       " \n",
       "         [[ 2.7372e-01,  6.7272e-02,  2.3239e-01,  ...,  2.5773e-01,\n",
       "           -3.0225e-01,  2.1616e-01],\n",
       "          [ 5.5857e-02, -1.0477e-02, -3.2290e-03,  ..., -1.6264e-01,\n",
       "           -1.0214e-03, -3.0844e-01],\n",
       "          [ 2.1490e-01,  6.3682e-02, -3.2594e-01,  ..., -3.3648e-01,\n",
       "           -7.0866e-02, -9.9481e-02],\n",
       "          ...,\n",
       "          [ 1.8768e-01, -4.2065e-01, -7.0882e-03,  ...,  1.7802e-01,\n",
       "            3.7622e-01, -1.3832e-01],\n",
       "          [-1.9358e-02,  1.9731e-01,  3.4328e-02,  ..., -2.8027e-01,\n",
       "            7.9868e-01, -6.8935e-02],\n",
       "          [-1.9837e-02,  5.5608e-01,  3.3288e-01,  ...,  2.7134e-01,\n",
       "            2.5464e-01,  2.0305e-01]]])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.pred_batch(batch=(xb,yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>lm_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17.509533</td>\n",
       "      <td>17.347189</td>\n",
       "      <td>0.139117</td>\n",
       "      <td>0.035498</td>\n",
       "      <td>01:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedder(vocab_sz, config['d_model'])\n",
    "bert_encoder = Encoder(**config)\n",
    "nw_encoder = Encoder(**config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=embed.embed, bias=False, output_p=False)\n",
    "bert_head = BertHead(embed, bert_encoder, nw_encoder, decoder)\n",
    "model = bert_head\n",
    "model.apply(init_transformer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_tfm(b):\n",
    "    x,y = b\n",
    "    return x, (y,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_dl.add_tfm(lm_tfm)\n",
    "data.valid_dl.add_tfm(lm_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='1170', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "__call__() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-486c2b8b9d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastai/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     21\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/fastai/fastai/basic_train.py\u001b[0m(30)\u001b[0;36mloss_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     28 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 30 \u001b[0;31m    \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     31 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     32 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> yb\n",
      "(tensor([[   1,    1,    1,  ..., 7180,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [   1,    1,    5,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,    5,    1,    1],\n",
      "        [   1,    1,  512,  ...,    1,    1,    1],\n",
      "        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0'), tensor([[    5, 25176,  2490,  ...,  7180,     5,  4539],\n",
      "        [    9,   299,   141,  ...,   335,    12,     9],\n",
      "        [   68,    11,     5,  ...,   695,    14,     5],\n",
      "        ...,\n",
      "        [    9,     5,   288,  ...,     5,     9,     5],\n",
      "        [19215,   188,   512,  ...,    15,     5, 21621],\n",
      "        [   11,     6,  3858,  ...,  2704,   912,    17]], device='cuda:0'))\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
