{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from fastai import *        # Quick accesss to most common functionality\n",
    "# from fastai.text import *   # Quick accesss to NLP functionality\n",
    "import html\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.transformer import tfmer_lm_config, GeLU, init_transformer, MultiHeadAttention, PositionalEncoding\n",
    "from fastai.text.models.awd_lstm import RNNDropout\n",
    "from fastai.text.learner import LanguageLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path.home()/'data/wikitext-2-raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basic_data import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 256\n",
    "data = load_data(PATH, bs=8, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = 'xxmask'\n",
    "vocab = data.vocab\n",
    "vocab.itos.append(MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_range = (0, len(data.vocab.itos))\n",
    "from fastai.text.transform import *\n",
    "pad_idx = data.vocab.stoi[PAD]\n",
    "mask_idx = data.vocab.stoi[MASK]\n",
    "def bert_tfm(b, word_range=word_range, pad_idx=pad_idx, \n",
    "             mask_idx=mask_idx, p=0.2):\n",
    "    # p = replacement probability\n",
    "    x_lm,y_lm = b\n",
    "    x_msk,y_msk = x_lm.clone(),x_lm.clone() # x, x\n",
    "#     x,y = x.clone(),y.clone()\n",
    "    rand = torch.rand(x_msk.shape, device=x_lm.device)\n",
    "    y_msk[rand > p] = pad_idx\n",
    "    x_msk[rand <= (p*.8)] = mask_idx # 80% = mask\n",
    "    wrong_word = (rand > (p*.8)) & (rand <= (p*.9)) # 10% = wrong word\n",
    "    x_msk[wrong_word] = torch.randint(*word_range, [wrong_word.sum().item()], device=x_lm.device)\n",
    "    return x_msk, y_msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_dl.tfms = [bert_tfm]\n",
    "data.valid_dl.tfms = [bert_tfm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc_ignore_pad(input:Tensor, targ:Tensor, pad_idx=pad_idx)->Rank0Tensor:\n",
    "    n = targ.shape[0]\n",
    "    input = input.argmax(dim=-1).view(n,-1)\n",
    "    targ = targ.view(n,-1)\n",
    "    mask = targ != pad_idx\n",
    "    return (input[mask]==targ[mask]).float().mean()\n",
    "\n",
    "def bert_acc(input:Tensor, b_t:Tensor, lm_t:Tensor)->Rank0Tensor:\n",
    "    return acc_ignore_pad(input, b_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearDecoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int, n_hid:int, tie_encoder:nn.Module=None, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs, outputs = input\n",
    "        decoded = self.decoder(outputs[-1])\n",
    "        return decoded, raw_outputs, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.mhra = MultiHeadAttention(n_heads, d_model, d_head, bias=bias)\n",
    "        self.ff   = feed_forward(d_model, d_inner)\n",
    "    \n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs): \n",
    "        attn = self.mhra(x, mask=mask, **kwargs)\n",
    "        res = self.ff(attn)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def feed_forward(d_model:int, d_ff:int, ff_p:float=0., act=nn.ReLU, double_drop:bool=True):\n",
    "def feed_forward(d_model:int, d_inner:int, ff_p:float=0.1):\n",
    "    layers = [\n",
    "        nn.Linear(d_model, d_inner), \n",
    "        GeLU(),\n",
    "        nn.Linear(d_inner, d_model), \n",
    "        nn.Dropout(ff_p), \n",
    "        MergeLayer(),\n",
    "        nn.LayerNorm(d_model)\n",
    "    ]\n",
    "    return SequentialEx(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleLayer(nn.Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, d_model:int, n_heads:int, d_head:int, ff_p:float=0.1, downsample=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mhra = MultiHeadAttention(n_heads, d_model, d_head, bias=True)\n",
    "        \n",
    "        d_inner = d_model*4\n",
    "        if downsample:\n",
    "            d_out = d_model * 2\n",
    "            self.downblock = nn.Conv1d(d_model, d_out, (2), stride=2)\n",
    "        else:\n",
    "            self.downblock = None\n",
    "            d_out = d_model\n",
    "            \n",
    "        self.ln1 = nn.Linear(d_out, d_inner)\n",
    "        self.act = GeLU()\n",
    "        self.ln2 = nn.Linear(d_inner, d_out)\n",
    "        self.drop = nn.Dropout(ff_p)\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "\n",
    "    \n",
    "    def forward(self, x:Tensor):\n",
    "        x_attn = self.mhra(x)\n",
    "        x = x_attn\n",
    "        \n",
    "        if self.downblock:\n",
    "            x_p = x.permute(0, 2, 1)\n",
    "            x_d = self.downblock(x_p) # bptt x emb x bptt\n",
    "            x = x_d.permute(0, 2, 1)\n",
    "        \n",
    "        x1 = self.ln2(self.act(self.ln1(x)))\n",
    "        \n",
    "        x2 = x + x1\n",
    "        \n",
    "        return self.norm(self.drop(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleLayer(nn.Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, d_model:int, n_heads:int, d_head:int, ff_p:float=0.1, upsample=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mhra = MultiHeadAttention(n_heads, d_model, d_head, bias=True)\n",
    "        \n",
    "        d_inner = d_model*4\n",
    "        if upsample:\n",
    "            d_out = d_model // 2\n",
    "            self.upblock = nn.ConvTranspose1d(d_model, d_out, (2), stride=2)\n",
    "        else:\n",
    "            d_out = d_model\n",
    "            self.upblock = None\n",
    "            \n",
    "        self.ln1 = nn.Linear(d_out, d_inner)\n",
    "        self.act = GeLU()\n",
    "        self.ln2 = nn.Linear(d_inner, d_out)\n",
    "        self.drop = nn.Dropout(ff_p)\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "\n",
    "    \n",
    "    def forward(self, x:Tensor, x_skip:Tensor):\n",
    "        x_attn = self.mhra(x)\n",
    "        x = x_attn\n",
    "        \n",
    "        if self.upblock:\n",
    "            x_p = x.permute(0, 2, 1)\n",
    "            x_u = self.upblock(x_p) # bptt x emb x bptt\n",
    "            x = x_u.permute(0, 2, 1)\n",
    "            \n",
    "        x1 = self.ln2(self.act(self.ln1(x)))\n",
    "        x2 = self.norm(self.drop(x1))\n",
    "        return x2 + x_skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt,d_model = 4, 64, 128\n",
    "# d1 = DecoderLayer(n_heads=4, d_model=d_model, d_head=32, d_inner=512)\n",
    "d1 = DownsampleLayer(n_heads=4, d_model=d_model, d_head=32, downsample=True)\n",
    "mask = torch.triu(torch.ones(bptt, bptt), diagonal=1).byte()[None,None]\n",
    "xb = torch.ones(bs,bptt,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 64, 128]), torch.Size([4, 32, 256]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, d1(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt,d_model = 4, 64, 128\n",
    "# d1 = DecoderLayer(n_heads=4, d_model=d_model, d_head=32, d_inner=512)\n",
    "u1 = UpsampleLayer(n_heads=4, d_model=d_model, d_head=32, upsample=True)\n",
    "xb = torch.ones(bs,bptt,d_model)\n",
    "x_skip = torch.ones(bs,bptt*2,d_model//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 64, 128]), torch.Size([4, 128, 64]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, x_skip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 64, 128]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.permute(0,2,1).shape, u1.upblock(xb.permute(0,2,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u1.upblock(xb.permute(0,2,1)).permute(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 64, 128]), torch.Size([4, 128, 64]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, u1(xb, x_skip).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape and half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]],\n",
       "\n",
       "        [[20, 21, 22, 23, 24],\n",
       "         [25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34],\n",
       "         [35, 36, 37, 38, 39]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(40).view(2, 4, 5); a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]],\n",
       "\n",
       "        [[20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(2, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = nn.Linear(d_model*2, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1d example -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 128, 32]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Conv1d(128, 128, kernel_size=(3), stride=2, padding=3//2)\n",
    "input = torch.randn(4, 128, 64)\n",
    "output = m(input)\n",
    "input.shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 64]), torch.Size([4, 128, 32]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Conv1d(128, 128, kernel_size=2, stride=2, padding=0)\n",
    "input = torch.randn(4, 128, 64)\n",
    "output = m(input)\n",
    "input.shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1d Transpose Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 8]), torch.Size([1, 5, 16]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.ConvTranspose1d(5, 5, (9), stride=1, padding=0)\n",
    "input = torch.randn(1, 5, 8)\n",
    "output = m(input)\n",
    "input.shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.ConvTranspose1d(128, 128, (2), stride=2)\n",
    "input = torch.randn(4, 128, 32)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128, 32]), torch.Size([4, 128, 64]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt,d_model = 4, 64, 128\n",
    "# d1 = DecoderLayer(n_heads=4, d_model=d_model, d_head=32, d_inner=512)\n",
    "mhra = MultiHeadAttention(n_heads=4, d_model=d_model, d_head=32)\n",
    "ff   = feed_forward(d_model=d_model, d_inner=512)\n",
    "\n",
    "mask = torch.triu(torch.ones(bptt, bptt), diagonal=1).byte()[None,None]\n",
    "xb = torch.ones(bs,bptt,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 1,  ..., 1, 1, 1],\n",
       "          [0, 0, 1,  ..., 1, 1, 1],\n",
       "          [0, 0, 0,  ..., 1, 1, 1],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 1, 1],\n",
       "          [0, 0, 0,  ..., 0, 0, 1],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 64, 128]), torch.Size([4, 64, 128]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_attn = mhra(xb, mask=mask)\n",
    "x_ff = ff(x_attn)\n",
    "x_attn.shape, x_ff.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, vocab_sz:int, ctx_len:int, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 embed_p:float=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Embedding(vocab_sz, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.drop_emb = nn.Dropout(embed_p)\n",
    "#         self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner) for k in range(n_layers)])\n",
    "        self.a1 = DownsampleLayer(d_model, n_heads, d_head)\n",
    "        self.a2 = DownsampleLayer(d_model*2, n_heads, d_head)\n",
    "        self.a3 = DownsampleLayer(d_model*4, n_heads, d_head, downsample=False)\n",
    "        self.a4 = UpsampleLayer(d_model*4, n_heads, d_head)\n",
    "        self.a5 = UpsampleLayer(d_model*2, n_heads, d_head)\n",
    "    \n",
    "    def reset(self): pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, x_len = x.size()\n",
    "        pos = torch.arange(0, x_len, device=x.device, dtype=x.dtype).float()\n",
    "        inp = self.drop_emb(self.encoder(x) + self.pos_enc(pos)[None]) #.mul_(self.d_model ** 0.5)\n",
    "#         mask = torch.triu(x.new_ones(x_len, x_len), diagonal=1).byte()[None,None]\n",
    "        #[None,:,:None] for einsum implementation of attention\n",
    "#         for layer in self.layers: inp = layer(inp, mask=mask)\n",
    "#         print('Inp:', inp.shape)\n",
    "        x1 = self.a1(inp)\n",
    "#         print('x1:', x1.shape)\n",
    "        x2 = self.a2(x1)\n",
    "#         print('x2:', x2.shape)\n",
    "        x3 = self.a3(x2)\n",
    "        x4 = self.a4(x3, x1)\n",
    "        x5 = self.a5(x4, inp)\n",
    "        \n",
    "#         print(x1.shape, x2.shape, x3.shape, x4.shape)\n",
    "        return ([x5],[x5]) #For the LinearDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sz = len(data.vocab.itos)\n",
    "# config = tfmer_lm_config.copy(); config\n",
    "config = {\n",
    "    'ctx_len': bptt,\n",
    "    'n_layers': 4,\n",
    "    'n_heads': 4,\n",
    "    'd_model': 128,\n",
    "    'd_head': 32,\n",
    "    'd_inner': 512,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16\n",
    "d_head = 4\n",
    "n_heads = 4\n",
    "emb = nn.Embedding(vocab_sz, d_model) \n",
    "a1 = DownsampleLayer(d_model, n_heads, d_head) \n",
    "a2 = DownsampleLayer(d_model*2, n_heads, d_head)\n",
    "a3 = DownsampleLayer(d_model*4, n_heads, d_head, downsample=False)\n",
    "a4 = UpsampleLayer(d_model*4, n_heads, d_head)\n",
    "a5 = UpsampleLayer(d_model*2, n_heads, d_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch(cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 256, 16])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = emb(xb); inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 32])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = a1(inp); x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 64])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = a2(x1); x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3 = a3(x2); x3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 128, 32])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4 = a4(x3, x1); x4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Transformer(\n",
       "    (encoder): Embedding(39881, 128)\n",
       "    (pos_enc): PositionalEncoding()\n",
       "    (drop_emb): Dropout(p=0.1)\n",
       "    (a1): DownsampleLayer(\n",
       "      (mhra): MultiHeadAttention(\n",
       "        (attention): Linear(in_features=128, out_features=384, bias=True)\n",
       "        (out): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (downblock): Conv1d(128, 256, kernel_size=(2,), stride=(2,))\n",
       "      (ln1): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (act): GeLU()\n",
       "      (ln2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (drop): Dropout(p=0.1)\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (a2): DownsampleLayer(\n",
       "      (mhra): MultiHeadAttention(\n",
       "        (attention): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (out): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (downblock): Conv1d(256, 512, kernel_size=(2,), stride=(2,))\n",
       "      (ln1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (act): GeLU()\n",
       "      (ln2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (drop): Dropout(p=0.1)\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (a3): DownsampleLayer(\n",
       "      (mhra): MultiHeadAttention(\n",
       "        (attention): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (out): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (ln1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (act): GeLU()\n",
       "      (ln2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (drop): Dropout(p=0.1)\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (a4): UpsampleLayer(\n",
       "      (mhra): MultiHeadAttention(\n",
       "        (attention): Linear(in_features=512, out_features=384, bias=True)\n",
       "        (out): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (upblock): ConvTranspose1d(512, 256, kernel_size=(2,), stride=(2,))\n",
       "      (ln1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "      (act): GeLU()\n",
       "      (ln2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "      (drop): Dropout(p=0.1)\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (a5): UpsampleLayer(\n",
       "      (mhra): MultiHeadAttention(\n",
       "        (attention): Linear(in_features=256, out_features=384, bias=True)\n",
       "        (out): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (upblock): ConvTranspose1d(256, 128, kernel_size=(2,), stride=(2,))\n",
       "      (ln1): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      (act): GeLU()\n",
       "      (ln2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      (drop): Dropout(p=0.1)\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=128, out_features=39881, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Transformer(vocab_sz, **config)\n",
    "decoder = LinearDecoder(vocab_sz, config['d_model'], tie_encoder=encoder.encoder, bias=False)\n",
    "model = nn.Sequential(encoder, decoder)\n",
    "model.reset = lambda: True\n",
    "model.apply(init_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1492, -0.1778,  0.1127,  ...,  0.4905,  0.2945,  0.2801],\n",
       "          [ 0.2332,  0.1924, -0.0607,  ...,  0.5226, -0.0131,  0.1304],\n",
       "          [-0.0346, -0.5041,  0.3645,  ..., -0.0707, -0.1599,  0.1048],\n",
       "          ...,\n",
       "          [-0.2043,  0.1740, -0.1442,  ..., -0.1785,  0.2258, -0.3501],\n",
       "          [-0.2488, -0.2424,  0.1111,  ..., -0.4120, -0.2610, -0.3826],\n",
       "          [-0.0871,  0.2557, -0.2549,  ...,  0.1698,  0.2015, -0.5001]],\n",
       " \n",
       "         [[ 0.4238, -0.3886,  0.3914,  ...,  0.4161, -0.0464,  0.2260],\n",
       "          [ 0.3174,  0.1404, -0.1269,  ...,  0.4123,  0.2232,  0.5896],\n",
       "          [-0.0503, -0.6630,  0.4550,  ..., -0.1371, -0.0544,  0.3313],\n",
       "          ...,\n",
       "          [-0.4075,  0.1626, -0.3683,  ..., -0.4111,  0.4170, -0.4185],\n",
       "          [ 0.4170,  0.3523,  0.2773,  ..., -0.4189,  0.0222, -0.1661],\n",
       "          [-0.1041,  0.0805, -0.0236,  ...,  0.1391,  0.4103, -0.4348]],\n",
       " \n",
       "         [[ 0.4861, -0.1117,  0.4745,  ...,  0.3369,  0.0152,  0.3588],\n",
       "          [ 0.1431, -0.0468, -0.1852,  ...,  0.6379,  0.2463,  0.2802],\n",
       "          [-0.0887, -0.4989,  0.5641,  ..., -0.2912, -0.3398,  0.1074],\n",
       "          ...,\n",
       "          [ 0.0831,  0.2756, -0.1018,  ..., -0.3766,  0.2090, -0.1956],\n",
       "          [ 0.0856,  0.1188,  0.0195,  ..., -0.4183, -0.1555, -0.3620],\n",
       "          [-0.0976,  0.0735, -0.0284,  ..., -0.1118,  0.0757, -0.2309]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.1101, -0.4183,  0.4520,  ...,  0.0304,  0.1005,  0.5124],\n",
       "          [ 0.1984, -0.0875,  0.1120,  ...,  0.4688,  0.1344,  0.2901],\n",
       "          [ 0.1009, -0.1653,  0.4288,  ..., -0.0105, -0.1150,  0.3268],\n",
       "          ...,\n",
       "          [-0.0889,  0.2021, -0.2427,  ..., -0.2279,  0.5802, -0.2552],\n",
       "          [ 0.1256,  0.0203,  0.1401,  ..., -0.4692, -0.2244, -0.1051],\n",
       "          [ 0.2008,  0.3712,  0.0369,  ...,  0.2166,  0.1973, -0.3025]],\n",
       " \n",
       "         [[ 0.1617, -0.0447,  0.2634,  ...,  0.4881,  0.2423,  0.4943],\n",
       "          [ 0.0091, -0.2154,  0.1068,  ...,  0.4501,  0.2695,  0.3463],\n",
       "          [-0.1708, -0.6240,  0.4062,  ..., -0.2731, -0.0856,  0.0150],\n",
       "          ...,\n",
       "          [-0.1760,  0.2537,  0.0282,  ..., -0.2108,  0.4353, -0.2149],\n",
       "          [ 0.0310, -0.0534,  0.1956,  ..., -0.2328, -0.0717, -0.3339],\n",
       "          [ 0.0335, -0.0040, -0.1446,  ..., -0.1372,  0.3977, -0.5141]],\n",
       " \n",
       "         [[ 0.0609, -0.3515,  0.0914,  ...,  0.1125,  0.2153,  0.5487],\n",
       "          [ 0.2541,  0.1329, -0.2015,  ...,  0.6026,  0.2708,  0.3136],\n",
       "          [-0.3672, -0.5660,  0.1578,  ..., -0.1562, -0.1617,  0.0637],\n",
       "          ...,\n",
       "          [ 0.0895,  0.4745,  0.0734,  ...,  0.0157,  0.4149, -0.2064],\n",
       "          [ 0.0957, -0.0751,  0.1175,  ..., -0.3528, -0.0357, -0.3008],\n",
       "          [-0.1053,  0.0805, -0.0453,  ..., -0.0940,  0.2097, -0.5304]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " [tensor([[[ 2.4556e-01,  1.4764e+00,  2.1187e-02,  ...,  7.6999e-01,\n",
       "             2.9951e+00,  1.2187e+00],\n",
       "           [-3.9992e-02,  8.9400e-01,  4.9320e-01,  ...,  6.1661e-01,\n",
       "             8.1170e-02,  2.7901e+00],\n",
       "           [ 8.7875e-01, -2.4416e-01,  1.4845e+00,  ...,  1.7648e+00,\n",
       "             2.8473e+00,  2.2955e-01],\n",
       "           ...,\n",
       "           [ 1.0270e+00,  1.5864e+00,  5.5890e-01,  ...,  9.9967e-01,\n",
       "             4.8667e-01, -2.6632e-01],\n",
       "           [-2.1508e+00, -1.1160e+00,  1.8719e+00,  ...,  2.5147e+00,\n",
       "            -5.7172e-03, -4.3566e-01],\n",
       "           [ 9.3865e-01,  1.0695e+00,  7.5600e-01,  ...,  1.0031e+00,\n",
       "             1.7200e-01,  1.0478e+00]],\n",
       "  \n",
       "          [[ 8.1836e-02,  1.5379e+00,  1.2124e+00,  ..., -6.5187e-01,\n",
       "             3.1029e+00,  5.1952e-01],\n",
       "           [ 1.4379e+00,  1.8408e+00,  1.3164e+00,  ..., -2.8338e-01,\n",
       "            -7.3129e-02,  1.0908e+00],\n",
       "           [-2.9671e-01,  1.8095e+00,  1.9533e+00,  ...,  1.3892e+00,\n",
       "             2.9127e+00,  1.6202e-01],\n",
       "           ...,\n",
       "           [ 1.5115e+00, -7.3139e-02, -3.5481e-01,  ...,  2.9366e+00,\n",
       "            -5.0853e-01, -2.0529e-01],\n",
       "           [-2.7861e+00, -3.2971e-01,  1.2603e+00,  ...,  2.8185e+00,\n",
       "            -7.0130e-01, -8.8756e-01],\n",
       "           [ 6.5791e-01,  1.2394e+00,  6.3058e-01,  ...,  2.0292e+00,\n",
       "             1.8629e+00,  2.0246e+00]],\n",
       "  \n",
       "          [[ 6.0209e-01,  8.4111e-01,  1.8515e-01,  ...,  1.1814e+00,\n",
       "             3.2483e+00,  7.8985e-01],\n",
       "           [ 1.1327e+00,  6.4107e-01,  2.8042e-01,  ...,  1.1407e+00,\n",
       "             1.1675e+00,  2.8031e+00],\n",
       "           [-8.7524e-01,  2.4444e-01,  1.4628e+00,  ...,  2.0518e+00,\n",
       "             1.1919e+00,  6.5957e-01],\n",
       "           ...,\n",
       "           [ 9.4947e-01, -3.6949e-01, -1.0975e-01,  ...,  2.1967e+00,\n",
       "            -7.8540e-01, -1.3244e+00],\n",
       "           [-2.0565e+00,  3.5607e-02,  3.1281e+00,  ...,  2.6257e+00,\n",
       "             4.7290e-01, -2.7297e+00],\n",
       "           [ 9.5199e-01,  5.1813e-01,  3.0865e-01,  ...,  2.0305e+00,\n",
       "             9.8521e-01,  1.7085e+00]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 6.9798e-01,  1.4883e+00,  5.3089e-01,  ...,  1.1737e+00,\n",
       "             3.4679e+00,  8.9328e-01],\n",
       "           [ 1.0040e+00,  1.6404e+00,  9.8085e-01,  ...,  2.7748e-01,\n",
       "             1.1298e+00,  1.9027e+00],\n",
       "           [-4.6292e-01,  1.2413e+00,  1.0528e+00,  ...,  1.3108e+00,\n",
       "             2.0559e+00, -1.2419e+00],\n",
       "           ...,\n",
       "           [-1.0641e+00, -9.7230e-01,  6.0314e-01,  ...,  3.2452e+00,\n",
       "            -6.5974e-01,  9.6592e-01],\n",
       "           [-2.3485e+00, -1.7559e+00,  2.2256e+00,  ...,  3.3387e+00,\n",
       "            -1.0664e+00, -1.0936e+00],\n",
       "           [ 5.2438e-01,  4.9816e-01,  6.4360e-01,  ...,  2.5141e+00,\n",
       "             9.7207e-01,  1.9216e+00]],\n",
       "  \n",
       "          [[ 3.9527e-02,  1.7504e+00,  9.8542e-01,  ...,  1.1686e+00,\n",
       "             1.1587e+00,  8.7127e-01],\n",
       "           [ 2.0898e+00,  1.2445e+00,  1.0446e+00,  ...,  1.0574e+00,\n",
       "             8.3335e-01,  2.6955e+00],\n",
       "           [ 4.1084e-01,  1.2952e+00,  1.6711e+00,  ...,  2.4163e+00,\n",
       "             2.7453e+00,  3.6158e-01],\n",
       "           ...,\n",
       "           [ 1.0289e+00, -2.8650e-01,  1.1116e+00,  ...,  2.6199e+00,\n",
       "             1.2805e+00, -5.2356e-01],\n",
       "           [-2.0136e+00, -6.6880e-01,  1.7756e+00,  ...,  1.2848e+00,\n",
       "            -8.3661e-01, -8.6871e-01],\n",
       "           [ 7.8499e-01,  9.0742e-01,  2.7872e-01,  ...,  3.0792e+00,\n",
       "             1.6247e+00,  2.0300e-03]],\n",
       "  \n",
       "          [[-2.1112e-01,  2.0714e+00,  4.0650e-01,  ...,  1.7285e+00,\n",
       "             1.1039e+00,  1.0154e-01],\n",
       "           [ 1.8396e+00,  1.9094e+00,  1.9305e+00,  ...,  1.0269e+00,\n",
       "             4.6146e-02,  3.1244e+00],\n",
       "           [ 1.0815e+00,  1.1699e+00,  2.2234e+00,  ...,  1.6673e+00,\n",
       "             1.9227e+00, -4.1263e-01],\n",
       "           ...,\n",
       "           [ 2.8483e-01,  1.1773e-01,  1.0023e+00,  ...,  3.3001e+00,\n",
       "             1.1524e+00,  4.6243e-01],\n",
       "           [-1.5824e+00, -1.1259e+00,  2.6440e+00,  ...,  2.4418e+00,\n",
       "             1.4696e-01, -1.2145e+00],\n",
       "           [ 6.4915e-01,  1.2979e+00, -1.8198e-01,  ...,  2.0305e+00,\n",
       "             2.9427e-01,  1.3407e+00]]], grad_fn=<AddBackward0>)],\n",
       " [tensor([[[ 2.4556e-01,  1.4764e+00,  2.1187e-02,  ...,  7.6999e-01,\n",
       "             2.9951e+00,  1.2187e+00],\n",
       "           [-3.9992e-02,  8.9400e-01,  4.9320e-01,  ...,  6.1661e-01,\n",
       "             8.1170e-02,  2.7901e+00],\n",
       "           [ 8.7875e-01, -2.4416e-01,  1.4845e+00,  ...,  1.7648e+00,\n",
       "             2.8473e+00,  2.2955e-01],\n",
       "           ...,\n",
       "           [ 1.0270e+00,  1.5864e+00,  5.5890e-01,  ...,  9.9967e-01,\n",
       "             4.8667e-01, -2.6632e-01],\n",
       "           [-2.1508e+00, -1.1160e+00,  1.8719e+00,  ...,  2.5147e+00,\n",
       "            -5.7172e-03, -4.3566e-01],\n",
       "           [ 9.3865e-01,  1.0695e+00,  7.5600e-01,  ...,  1.0031e+00,\n",
       "             1.7200e-01,  1.0478e+00]],\n",
       "  \n",
       "          [[ 8.1836e-02,  1.5379e+00,  1.2124e+00,  ..., -6.5187e-01,\n",
       "             3.1029e+00,  5.1952e-01],\n",
       "           [ 1.4379e+00,  1.8408e+00,  1.3164e+00,  ..., -2.8338e-01,\n",
       "            -7.3129e-02,  1.0908e+00],\n",
       "           [-2.9671e-01,  1.8095e+00,  1.9533e+00,  ...,  1.3892e+00,\n",
       "             2.9127e+00,  1.6202e-01],\n",
       "           ...,\n",
       "           [ 1.5115e+00, -7.3139e-02, -3.5481e-01,  ...,  2.9366e+00,\n",
       "            -5.0853e-01, -2.0529e-01],\n",
       "           [-2.7861e+00, -3.2971e-01,  1.2603e+00,  ...,  2.8185e+00,\n",
       "            -7.0130e-01, -8.8756e-01],\n",
       "           [ 6.5791e-01,  1.2394e+00,  6.3058e-01,  ...,  2.0292e+00,\n",
       "             1.8629e+00,  2.0246e+00]],\n",
       "  \n",
       "          [[ 6.0209e-01,  8.4111e-01,  1.8515e-01,  ...,  1.1814e+00,\n",
       "             3.2483e+00,  7.8985e-01],\n",
       "           [ 1.1327e+00,  6.4107e-01,  2.8042e-01,  ...,  1.1407e+00,\n",
       "             1.1675e+00,  2.8031e+00],\n",
       "           [-8.7524e-01,  2.4444e-01,  1.4628e+00,  ...,  2.0518e+00,\n",
       "             1.1919e+00,  6.5957e-01],\n",
       "           ...,\n",
       "           [ 9.4947e-01, -3.6949e-01, -1.0975e-01,  ...,  2.1967e+00,\n",
       "            -7.8540e-01, -1.3244e+00],\n",
       "           [-2.0565e+00,  3.5607e-02,  3.1281e+00,  ...,  2.6257e+00,\n",
       "             4.7290e-01, -2.7297e+00],\n",
       "           [ 9.5199e-01,  5.1813e-01,  3.0865e-01,  ...,  2.0305e+00,\n",
       "             9.8521e-01,  1.7085e+00]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[ 6.9798e-01,  1.4883e+00,  5.3089e-01,  ...,  1.1737e+00,\n",
       "             3.4679e+00,  8.9328e-01],\n",
       "           [ 1.0040e+00,  1.6404e+00,  9.8085e-01,  ...,  2.7748e-01,\n",
       "             1.1298e+00,  1.9027e+00],\n",
       "           [-4.6292e-01,  1.2413e+00,  1.0528e+00,  ...,  1.3108e+00,\n",
       "             2.0559e+00, -1.2419e+00],\n",
       "           ...,\n",
       "           [-1.0641e+00, -9.7230e-01,  6.0314e-01,  ...,  3.2452e+00,\n",
       "            -6.5974e-01,  9.6592e-01],\n",
       "           [-2.3485e+00, -1.7559e+00,  2.2256e+00,  ...,  3.3387e+00,\n",
       "            -1.0664e+00, -1.0936e+00],\n",
       "           [ 5.2438e-01,  4.9816e-01,  6.4360e-01,  ...,  2.5141e+00,\n",
       "             9.7207e-01,  1.9216e+00]],\n",
       "  \n",
       "          [[ 3.9527e-02,  1.7504e+00,  9.8542e-01,  ...,  1.1686e+00,\n",
       "             1.1587e+00,  8.7127e-01],\n",
       "           [ 2.0898e+00,  1.2445e+00,  1.0446e+00,  ...,  1.0574e+00,\n",
       "             8.3335e-01,  2.6955e+00],\n",
       "           [ 4.1084e-01,  1.2952e+00,  1.6711e+00,  ...,  2.4163e+00,\n",
       "             2.7453e+00,  3.6158e-01],\n",
       "           ...,\n",
       "           [ 1.0289e+00, -2.8650e-01,  1.1116e+00,  ...,  2.6199e+00,\n",
       "             1.2805e+00, -5.2356e-01],\n",
       "           [-2.0136e+00, -6.6880e-01,  1.7756e+00,  ...,  1.2848e+00,\n",
       "            -8.3661e-01, -8.6871e-01],\n",
       "           [ 7.8499e-01,  9.0742e-01,  2.7872e-01,  ...,  3.0792e+00,\n",
       "             1.6247e+00,  2.0300e-03]],\n",
       "  \n",
       "          [[-2.1112e-01,  2.0714e+00,  4.0650e-01,  ...,  1.7285e+00,\n",
       "             1.1039e+00,  1.0154e-01],\n",
       "           [ 1.8396e+00,  1.9094e+00,  1.9305e+00,  ...,  1.0269e+00,\n",
       "             4.6146e-02,  3.1244e+00],\n",
       "           [ 1.0815e+00,  1.1699e+00,  2.2234e+00,  ...,  1.6673e+00,\n",
       "             1.9227e+00, -4.1263e-01],\n",
       "           ...,\n",
       "           [ 2.8483e-01,  1.1773e-01,  1.0023e+00,  ...,  3.3001e+00,\n",
       "             1.1524e+00,  4.6243e-01],\n",
       "           [-1.5824e+00, -1.1259e+00,  2.6440e+00,  ...,  2.4418e+00,\n",
       "             1.4696e-01, -1.2145e+00],\n",
       "           [ 6.4915e-01,  1.2979e+00, -1.8198e-01,  ...,  2.0305e+00,\n",
       "             2.9427e-01,  1.3407e+00]]], grad_fn=<AddBackward0>)])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = LanguageLearner(data, model, loss_func=CrossEntropyFlat(ignore_index=pad_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics = [bert_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.366689</td>\n",
       "      <td>6.109301</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>01:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
