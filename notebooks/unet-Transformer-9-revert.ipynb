{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from fastai import *        # Quick accesss to most common functionality\n",
    "# from fastai.text import *   # Quick accesss to NLP functionality\n",
    "import html\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.models.transformer import *\n",
    "from fastai.text.models.transformer import init_transformer\n",
    "from fastai.text.models.awd_lstm import RNNDropout, LinearDecoder\n",
    "from fastai.text.learner import LanguageLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path.home()/'data/wikitext-2-raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basic_data import *\n",
    "from fastai.torch_core import *\n",
    "from fastai.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 512\n",
    "data = load_data(PATH, bs=16, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = 'xxmask'\n",
    "vocab = data.vocab\n",
    "vocab.itos.append(MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_range = (0, len(data.vocab.itos))\n",
    "from fastai.text.transform import *\n",
    "pad_idx = data.vocab.stoi[PAD]\n",
    "mask_idx = data.vocab.stoi[MASK]\n",
    "def bert_tfm(b, word_range=word_range, pad_idx=pad_idx, \n",
    "             mask_idx=mask_idx, p=0.2):\n",
    "    # p = replacement probability\n",
    "    x_lm,y_lm = b\n",
    "    x_msk,y_msk = x_lm.clone(),x_lm.clone() # x, x\n",
    "    rand = torch.rand(x_msk.shape, device=x_lm.device)\n",
    "    y_msk[rand > p] = pad_idx\n",
    "    x_msk[rand <= (p*.8)] = mask_idx # 80% = mask\n",
    "    wrong_word = (rand > (p*.8)) & (rand <= (p*.9)) # 10% = wrong word\n",
    "    x_msk[wrong_word] = torch.randint(*word_range, [wrong_word.sum().item()], device=x_lm.device)\n",
    "    return x_msk, y_msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_dl.tfms = [bert_tfm]\n",
    "data.valid_dl.tfms = [bert_tfm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc_ignore_pad(input:Tensor, targ:Tensor, pad_idx=pad_idx)->Rank0Tensor:\n",
    "    n = targ.shape[0]\n",
    "    input = input.argmax(dim=-1).view(n,-1)\n",
    "    targ = targ.view(n,-1)\n",
    "    mask = targ != pad_idx\n",
    "    return (input[mask]==targ[mask]).float().mean()\n",
    "\n",
    "def bert_acc(input:Tensor, b_t:Tensor)->Rank0Tensor:\n",
    "    return acc_ignore_pad(input, b_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int, n_hid:int, output_p:float=0.0, tie_encoder:nn.Module=None, bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        output = self.output_dp(input)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, d_attn:int, n_heads:int, d_head:int, d_out:int=None, bias:bool=True, ff_p=0.0):\n",
    "        super().__init__()\n",
    "        if d_out is None: d_out = d_attn\n",
    "        d_inner = d_out*4\n",
    "        self.mhra = MultiHeadRelativeAttention(n_heads, d_attn, d_head, bias=bias)\n",
    "        \n",
    "#         self.mhra = MultiHeadAttention(n_heads, d_attn, d_head, bias=bias)\n",
    "        self.pos_enc = PositionalEncoding(d_attn)\n",
    "        self.u = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.v = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        \n",
    "        nn.init.normal_(self.u, 0., 0.02)\n",
    "        nn.init.normal_(self.v, 0., 0.02)\n",
    "        \n",
    "        self.ln = nn.Sequential(*[\n",
    "            nn.Linear(d_out, d_inner), \n",
    "            GeLU(),\n",
    "            nn.Linear(d_inner, d_out),\n",
    "            nn.Dropout(ff_p), \n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "        \n",
    "    def rel_attn(self, x:Tensor, mask:Tensor=None):\n",
    "#         return self.mhra(x)\n",
    "        pos = torch.arange(x.shape[1]-1, -1, -1, device=x.device, dtype=self.u.dtype)\n",
    "        pos_enc = self.pos_enc(pos)\n",
    "        return self.mhra(x, mask=mask, r=pos_enc, u=self.u, v=self.v)\n",
    "    \n",
    "    def forward(self, x:Tensor, mask:Tensor=None, **kwargs): \n",
    "        attn = self.rel_attn(x, mask=mask)\n",
    "        res = self.ln(attn) + attn\n",
    "        return self.norm(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock2(nn.Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, d_attn:int, n_heads:int, d_head:int, d_out:int=None, bias:bool=False, ff_p=0.0):\n",
    "        super().__init__()\n",
    "        if d_out is None: d_out = d_attn\n",
    "        d_inner = d_out*4\n",
    "        self.mhra = MultiHeadRelativeAttention(n_heads, d_attn, d_head, bias=bias)\n",
    "#         self.mhra = MultiHeadAttention(n_heads, d_attn, d_head, bias=bias)\n",
    "\n",
    "        self.ln = nn.Sequential(*[\n",
    "            nn.Linear(d_out, d_inner), \n",
    "            GeLU(),\n",
    "            nn.Linear(d_inner, d_out),\n",
    "            nn.Dropout(ff_p), \n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_out)\n",
    "        \n",
    "    def forward(self, x:Tensor, **kwargs): \n",
    "        attn = self.mhra(x, **kwargs)\n",
    "        res = self.ln(attn) + attn\n",
    "        return self.norm(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsampleLayer(TransformerBlock):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, d_model:int, n_heads:int, d_head:int):\n",
    "        d_out = d_model * 2\n",
    "        super().__init__(d_model, n_heads, d_head, d_out)\n",
    "        self.downblock = nn.Conv1d(d_model, d_out, (2), stride=2)\n",
    "            \n",
    "    def forward(self, x:Tensor):\n",
    "        x_attn = self.rel_attn(x)\n",
    "        \n",
    "        x_d = self.downblock(x_attn.permute(0, 2, 1)).permute(0, 2, 1) # bptt x emb x bptt\n",
    "        \n",
    "        x1 = self.ln(x_d)\n",
    "        \n",
    "        return self.norm(x_attn + x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleLayer(nn.Module):\n",
    "    \"Basic block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, d_model:int, n_heads:int, d_head:int):\n",
    "        d_out = d_model // 2\n",
    "        super().__init__(d_model, n_heads, d_head, d_out)\n",
    "        self.upblock = nn.ConvTranspose1d(d_model, d_out, (2), stride=2)\n",
    "\n",
    "    \n",
    "    def forward(self, x:Tensor, x_skip:Tensor, r=None):\n",
    "        x_attn = self.rel_attn(x)\n",
    "        \n",
    "        x_u = self.upblock(x_attn.permute(0, 2, 1)).permute(0, 2, 1) # bptt x emb x bptt\n",
    "        \n",
    "        x1 = self.ln(x_u)\n",
    "        \n",
    "        return self.norm(x1 + x_skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerConv(nn.Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, vocab_sz:int, ctx_len:int, n_layers:int, n_heads:int, d_model:int, d_head:int, \n",
    "                 embed_p:float=0.0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Embedding(vocab_sz, d_model)\n",
    "        self.drop_emb = nn.Dropout(embed_p)\n",
    "        # See https://arxiv.org/abs/1711.09160\n",
    "        with torch.no_grad(): trunc_normal_(self.encoder.weight, std=0.01)\n",
    "            \n",
    "#         self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner) for k in range(n_layers)])\n",
    "        self.a1 = DownsampleLayer(d_model, n_heads, d_head)\n",
    "        self.a2 = DownsampleLayer(d_model*2, n_heads, d_head)\n",
    "        self.a3 = DownsampleLayer(d_model*4, n_heads, d_head, downsample=False)\n",
    "        self.a4 = UpsampleLayer(d_model*4, n_heads, d_head)\n",
    "        self.a5 = UpsampleLayer(d_model*2, n_heads, d_head)\n",
    "    \n",
    "    def reset(self): pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, x_len = x.size()\n",
    "        inp = self.drop_emb(self.encoder(x)) #.mul_(self.d_model ** 0.5)\n",
    "#         mask = torch.triu(x.new_ones(x_len, x_len), diagonal=1).byte()[None,None]\n",
    "        #[None,:,:None] for einsum implementation of attention\n",
    "#         for layer in self.layers: inp = layer(inp, mask=mask)\n",
    "#         print('Inp:', inp.shape)\n",
    "        x1 = self.a1(inp)\n",
    "#         print('x1:', x1.shape)\n",
    "        x2 = self.a2(x1)\n",
    "#         print('x2:', x2.shape)\n",
    "        x3 = self.a3(x2)\n",
    "        x4 = self.a4(x3, x1)\n",
    "        x5 = self.a5(x4, inp)\n",
    "        \n",
    "#         print(x1.shape, x2.shape, x3.shape, x4.shape)\n",
    "        return x5 #For the LinearDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBase(nn.Module):\n",
    "    \"Transformer model: https://arxiv.org/abs/1706.03762.\"\n",
    "    def __init__(self, vocab_sz:int, ctx_len:int, n_layers:int, n_heads:int, d_model:int, d_head:int, \n",
    "                 embed_p:float=0.0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Embedding(vocab_sz, d_model)\n",
    "        # See https://arxiv.org/abs/1711.09160\n",
    "        with torch.no_grad(): trunc_normal_(self.encoder.weight, std=0.01)\n",
    "            \n",
    "        self.drop_emb = nn.Dropout(embed_p)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_model, d_head, n_heads) for k in range(n_layers)])\n",
    "    \n",
    "    def reset(self): pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, x_len = x.size()\n",
    "        inp = self.drop_emb(self.encoder(x)) #.mul_(self.d_model ** 0.5)\n",
    "#         mask = torch.triu(x.new_ones(x_len, x_len), diagonal=1).byte()[None,None]\n",
    "        for layer in self.layers: inp = layer(inp)\n",
    "        return inp #For the LinearDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sz = len(data.vocab.itos)\n",
    "# config = tfmer_lm_config.copy(); config\n",
    "config = {\n",
    "    'ctx_len': bptt,\n",
    "    'n_layers': 5,\n",
    "    'n_heads': 4,\n",
    "    'd_model': 128,\n",
    "    'd_head': 32,\n",
    "    'embed_p': 0.0 # Embed p needs to be 0 to match bert-mask baseline\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb,yb = data.one_batch(cpu=False)\n",
    "# model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Fastai TXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.learner import get_language_model\n",
    "encoder = TransformerXL(vocab_sz, d_inner=config['d_model']*4, **config)\n",
    "decoder = LinearDecoder(vocab_sz, config['d_model'], output_p=0.0, tie_encoder=encoder.encoder)\n",
    "model = nn.Sequential(encoder, decoder)\n",
    "model.reset = lambda: True\n",
    "model.apply(init_transformer)\n",
    "\n",
    "learn = LanguageLearner(data, model, loss_func=CrossEntropyFlat(ignore_index=pad_idx))\n",
    "learn.metrics = [bert_acc]\n",
    "# learn.to_fp16();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.600806</td>\n",
       "      <td>6.338635</td>\n",
       "      <td>0.141968</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.386663</td>\n",
       "      <td>6.154504</td>\n",
       "      <td>0.143694</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Transformer Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerBase(vocab_sz, **config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=encoder.encoder)\n",
    "model = nn.Sequential(encoder, decoder)\n",
    "model.reset = lambda: True\n",
    "model.apply(init_transformer)\n",
    "learn = LanguageLearner(data, model, loss_func=CrossEntropyFlat(ignore_index=pad_idx))\n",
    "learn.callbacks = []\n",
    "learn.metrics = [bert_acc]\n",
    "# learn.to_fp16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.611427</td>\n",
       "      <td>6.318287</td>\n",
       "      <td>0.143721</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.526922</td>\n",
       "      <td>6.277451</td>\n",
       "      <td>0.141834</td>\n",
       "      <td>01:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.577472</td>\n",
       "      <td>6.347648</td>\n",
       "      <td>0.142532</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.522913</td>\n",
       "      <td>6.308616</td>\n",
       "      <td>0.140825</td>\n",
       "      <td>00:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.491513</td>\n",
       "      <td>6.122727</td>\n",
       "      <td>0.163134</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.781649</td>\n",
       "      <td>5.540959</td>\n",
       "      <td>0.247909</td>\n",
       "      <td>01:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find(num_it=300)\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity - Bert Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conf transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder(nn.Module):\n",
    "    \"Embedding + positional encoding + dropout\"\n",
    "    def __init__(self, vocab_sz:int, emb_sz:int, embed_p:float=0., pad_idx=pad_idx):\n",
    "        super().__init__()\n",
    "        self.emb_sz = emb_sz\n",
    "        \n",
    "#         self.embed = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_idx)        \n",
    "        self.embed = nn.Embedding(vocab_sz, emb_sz)\n",
    "        # See https://arxiv.org/abs/1711.09160\n",
    "        with torch.no_grad(): trunc_normal_(self.embed.weight, std=0.01)\n",
    "        self.drop = nn.Dropout(embed_p)\n",
    "    \n",
    "    def forward(self, inp, pos_forward=False):\n",
    "        emb = self.drop(self.embed(inp))\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"TransformerXL model: https://arxiv.org/abs/1901.02860.\"\n",
    "    def __init__(self, n_layers:int, n_heads:int, d_model:int, d_head:int, d_inner:int, \n",
    "                 resid_p:float=0., attn_p:float=0., ff_p:float=0., bias:bool=False, scale:bool=True,\n",
    "                 act:Activation=Activation.GeLU, double_drop:bool=True, attn_cls:Callable=MultiHeadRelativeAttention,\n",
    "                 learned_pos_enc:bool=False, mask:bool=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.u = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.v = nn.Parameter(torch.Tensor(n_heads, 1, d_head)) #Remove 1 for einsum implementation of attention\n",
    "        self.n_layers,self.d_model,self.mask = n_layers,d_model,mask\n",
    "#         self.layers = nn.ModuleList([DecoderLayer(n_heads, d_model, d_head, d_inner, resid_p=resid_p, attn_p=attn_p,\n",
    "#                       ff_p=ff_p, bias=bias, scale=scale, act=act, double_drop=double_drop, \n",
    "#                       attn_cls=attn_cls) for k in range(n_layers)])\n",
    "        self.layers = nn.ModuleList([TransformerBlock2(d_model, n_heads, d_head, bias=bias, \n",
    "                      ) for k in range(n_layers)])\n",
    "        \n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        \n",
    "        nn.init.normal_(self.u, 0., 0.02)\n",
    "        nn.init.normal_(self.v, 0., 0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #The hidden state has to be initiliazed in the forward pass for nn.DataParallel\n",
    "        bs,x_len,emb_sz = x.size()\n",
    "        \n",
    "        inp = x\n",
    "        \n",
    "        pos = torch.arange(x_len-1, -1, -1, device=inp.device, dtype=inp.dtype)\n",
    "        pos_enc = self.pos_enc(pos)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            inp = layer(inp, r=pos_enc, u=self.u, v=self.v)\n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedder(vocab_sz, config['d_model'])\n",
    "encoder = Encoder(d_inner=config['d_model']*4, **config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=embed.embed)\n",
    "model = nn.Sequential(embed, encoder, decoder)\n",
    "model.reset = lambda: True\n",
    "model.apply(init_transformer)\n",
    "learn = LanguageLearner(data, model, loss_func=CrossEntropyFlat(ignore_index=pad_idx))\n",
    "learn.callbacks = []\n",
    "learn.metrics = [bert_acc]\n",
    "# learn.to_fp16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.224000</td>\n",
       "      <td>5.695016</td>\n",
       "      <td>0.220923</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.153956</td>\n",
       "      <td>4.931585</td>\n",
       "      <td>0.329466</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.079302</td>\n",
       "      <td>5.548858</td>\n",
       "      <td>0.251355</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.276506</td>\n",
       "      <td>5.089050</td>\n",
       "      <td>0.307184</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.616300</td>\n",
       "      <td>6.340132</td>\n",
       "      <td>0.142449</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.531385</td>\n",
       "      <td>6.317050</td>\n",
       "      <td>0.139587</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.584211</td>\n",
       "      <td>6.247587</td>\n",
       "      <td>0.139904</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.103616</td>\n",
       "      <td>5.864691</td>\n",
       "      <td>0.190566</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.179782</td>\n",
       "      <td>5.618222</td>\n",
       "      <td>0.236285</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.127200</td>\n",
       "      <td>4.915281</td>\n",
       "      <td>0.332678</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.046856</td>\n",
       "      <td>5.586289</td>\n",
       "      <td>0.232654</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.476103</td>\n",
       "      <td>5.239614</td>\n",
       "      <td>0.268202</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/2 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='266' class='' max='293', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      90.78% [266/293 00:57<00:05 6.5628]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerConv(vocab_sz, **config)\n",
    "decoder = Decoder(vocab_sz, config['d_model'], tie_encoder=encoder.encoder)\n",
    "model = nn.Sequential(encoder, decoder)\n",
    "model.reset = lambda: True\n",
    "model.apply(init_transformer)\n",
    "learn = LanguageLearner(data, model, loss_func=CrossEntropyFlat(ignore_index=pad_idx))\n",
    "learn.metrics = [bert_acc]\n",
    "# learn.to_fp16();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>bert_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.691076</td>\n",
       "      <td>6.477333</td>\n",
       "      <td>0.142961</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.534302</td>\n",
       "      <td>6.275502</td>\n",
       "      <td>0.179262</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.330779</td>\n",
       "      <td>6.108613</td>\n",
       "      <td>0.191137</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.290344</td>\n",
       "      <td>6.085637</td>\n",
       "      <td>0.190940</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(4, 1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
